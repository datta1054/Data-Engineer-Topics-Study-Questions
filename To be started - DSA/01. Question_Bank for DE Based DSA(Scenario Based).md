Great — below are 40 scenario-based DSA questions tailored for Data Engineering / Data Platform interviews (3–4 YOE). I’ve organized them by difficulty (10 Easy, 20 Medium, 10 Hard). Each question is written in a realistic interview style and contains:

Title

Difficulty

Primary Data Structure(s)

Key Algorithm / Concept

Detailed scenario & problem statement

Input / Output expectations

Constraints / scale notes

Hints / partial guidance

Real-world relevance for Data Engineering

Complexity targets (time / space)

Common edge cases & possible follow-ups

You can use these as mock interview prompts, whiteboard problems, or to practice framing answers that show both algorithmic thinking and data-engineering tradeoffs (memory, streaming, distributed handling). Let’s dive in.

Easy — 10 Scenario Questions
1) Deduplicate a small daily ingest (first occurrence)

Difficulty: Easy

Primary DS: HashMap / Set

Key Concept: Hashing, one-pass streaming

Scenario: Every day an ingestion job reads a CSV of event records (id, timestamp, payload). You need to produce a cleaned file containing only the first occurrence of each id for that day (keep earliest timestamp). Memory is small but the daily file fits in a few million rows.

Problem: Implement function that returns the subset containing the earliest record per id.

Input/Output: Input: list of records (id, timestamp, fields...) in arbitrary order. Output: list of records with unique id where timestamp is earliest.

Constraints: Single-machine; daily file up to ~5M rows; memory ~4–8 GB.

Hint: Use a HashMap from id → record (or min timestamp) and update on the fly. If records are streamed, write out finalized results at the end.

Real-world relevance: Deduping nearline batch ingests before loading into data warehouse.

Target complexity: O(n) time, O(u) space (u = unique ids).

Edge cases / Follow-ups: If memory insufficient, propose external sort or partition by hash (shuffle), or use probabilistic dedupe (Bloom filters) to reduce state.

2) Sliding window request counter (fixed window)

Difficulty: Easy

Primary DS: Queue / Circular buffer

Key Concept: Sliding window, timestamp queue

Scenario: You must implement a rate limiter that tracks number of requests per user_id in the last T seconds and returns true if below threshold k. The service is single-instance for the test.

Problem: Build allow_request(user_id, timestamp) returning boolean.

Input/Output: Calls arrive streaming with increasing timestamps. Return true/false for each call.

Constraints: T up to 60 seconds, k up to 1000, users moderate (10k concurrently).

Hint: Keep per-user deque of timestamps; pop old timestamps <= timestamp − T, check length. Use size limiting.

Real-world relevance: Throttling API or job submissions at ingestion layer.

Target complexity: amortized O(1) per request, O(k × active_users) memory.

Edge cases / Follow-ups: How to scale to distributed environment — use token bucket in Redis or leaky bucket, approximate counters (fixed window vs sliding window with buckets).

3) Find missing integer in small ID range

Difficulty: Easy

Primary DS: Bitset / Boolean array

Key Concept: Counting / presence tracking

Scenario: You receive a file with n unique integers in range [1..n+1] except one missing. Find the missing integer.

Problem: Return the missing integer.

Input/Output: Input list of n integers; output single integer.

Constraints: n up to 5M. Memory should be optimized.

Hint: XOR all numbers 1..n+1 with the inputs, or sum with careful overflow handling. For memory-limited case, bitset of n bits.

Real-world relevance: Detect missing sequence IDs in logs/partitions.

Target complexity: O(n) time, O(1) or O(n/8) space for bitset.

Edge cases / Follow-ups: If range huge (sparse), use hashing or external sort.

4) Merge two sorted daily snapshots (in-place)

Difficulty: Easy

Primary DS: Array / Two pointers

Key Concept: Merge in linear pass

Scenario: Two sorted files (by primary key) need merging into one ordered file. One file has extra free space at the end to do in-place merge.

Problem: Merge arrays A and B into A (in place).

Input/Output: Two sorted arrays with sizes m, n; A has size m+n with last n empty. Output A merged.

Constraints: Single-machine, do not allocate additional arrays.

Hint: Use two pointers from ends, write from back.

Real-world relevance: Merge incremental snapshots into master without extra IO.

Target complexity: O(m+n), O(1) extra space.

Edge cases / Follow-ups: Handle duplicates, stable merges if payload order matters.

5) Count distinct in a small window (exact)

Difficulty: Easy

Primary DS: HashSet, Sliding window

Key Concept: Windowed distinct count

Scenario: Given an array of events and window size k, return the number of distinct user IDs in every sliding window of size k. Exact answers required.

Problem: Return list of distinct counts per window start.

Input/Output: Array of user_ids; integer k. Output list length n−k+1.

Constraints: n up to 1M, k up to 10k.

Hint: Maintain counts HashMap for current window, increment/decrement as window slides.

Real-world relevance: Exact unique user counts for small windows in nearline analytics.

Target complexity: O(n), O(u_in_window).

Edge cases / Follow-ups: If approximate counts acceptable, propose HyperLogLog for larger windows.

6) Reverse the columns of a CSV row

Difficulty: Easy

Primary DS: Array / String

Key Concept: In-place reversal, parsing

Scenario: For a transformation, you need to reverse the order of columns in a CSV row efficiently (without full re-parsing heavy transformations).

Problem: Given a CSV line (simple—no escapes or quotes), return columns reversed.

Input/Output: "a,b,c,d" → "d,c,b,a".

Constraints: Row width small (<= 10k chars).

Hint: Split on comma or parse from end collecting tokens. Try avoid creating many intermediate strings if very long — use buffer and reverse indexes.

Real-world relevance: Column reordering in ETL or for compact previews.

Target complexity: O(n) time, O(n) space (unless in-place buffer).

Edge cases / Follow-ups: Handling quoted fields, escaped commas — need full CSV parser.

7) Validate JSON-like bracket matching

Difficulty: Easy

Primary DS: Stack

Key Concept: Bracket matching, stack usage

Scenario: Lightweight validator to check that braces/brackets/parentheses in JSON payload are balanced (not validating full JSON).

Problem: Return true/false if brackets are balanced and properly nested.

Input/Output: String payload → boolean.

Constraints: Payload size up to a few MB.

Hint: Push opening bracket, pop and check matching closing bracket. Ignore characters inside quotes for more accuracy (optional).

Real-world relevance: Quick sanity check before parsing or storing JSON logs.

Target complexity: O(n) time, O(depth) space.

Edge cases / Follow-ups: Handle escaped quotes, strings, comments if present.

8) Partition events by day

Difficulty: Easy

Primary DS: HashMap of lists (or streaming write to daily files)

Key Concept: Partitioning, hashing by key (date)

Scenario: Given a batch of events with timestamps, partition them into daily buckets writing to separate files (or buffers). Keep events for each day in order of arrival.

Problem: Produce mapping day → list of events or simulate writing to files.

Input/Output: List of events with timestamps → map/disk files per day.

Constraints: Number of distinct days small (e.g., 7). Memory moderate.

Hint: Compute date key and append to corresponding buffer; flush if buffer big.

Real-world relevance: Partitioning data for downstream storage like date-partitioned tables.

Target complexity: O(n) time, O(sum buffers) space.

Edge cases / Follow-ups: Streamed input, ordering guarantees across partitions — consider sequence numbers.

9) Rotate array by k (cyclic shift)

Difficulty: Easy

Primary DS: Array

Key Concept: Array rotation, reverse trick

Scenario: Shift a large in-memory array right by k positions in place, used to align time series.

Problem: Modify array to rotated version.

Input/Output: [1,2,3,4,5], k=2 → [4,5,1,2,3].

Constraints: Do it in place, no extra array.

Hint: Use triple reverse: reverse whole array, reverse first k, reverse rest.

Real-world relevance: Time alignment, circular buffer indexing.

Target complexity: O(n) time, O(1) extra space.

Edge cases / Follow-ups: Negative or large k (mod n). Streaming rotation for huge datasets — rotate via disk blocks.

10) Count zeros in binary representation across IDs

Difficulty: Easy

Primary DS: Bit operations

Key Concept: Bit counting / popcount complement

Scenario: For storage optimization, you want to count number of zero bits across a list of 32-bit IDs to measure sparsity. Return counts per ID and global zero count.

Problem: Given list of ints, output list of zero-bit counts and total zeros.

Input/Output: [3 (0b11), 5 (101)] → per-id zeros [30, 30] (if 32-bit), total 60.

Constraints: Up to millions of ints; do bit ops efficiently.

Hint: Use built-in popcount for ones and subtract from fixed width, vectorize if possible.

Real-world relevance: Feature sparsity checks before encoding.

Target complexity: O(n).

Edge cases / Follow-ups: Variable bit widths, signed vs unsigned.

Medium — 20 Scenario Questions
11) Merge multiple sorted partition files (K-way merge)

Difficulty: Medium

Primary DS: Heap (priority queue)

Key Concept: K-way merge, external merge sort

Scenario: You have K sorted partition files (each is sorted by timestamp) produced by distributed mappers. You need to merge them into a single sorted output stream for loading into a time-ordered table. Files are large (each may be many GB).

Problem: Implement external K-way merge logic (conceptually) that produces a merged stream. Discuss memory vs IO tradeoffs.

Input/Output: K file handles/iterators → merged stream iterator.

Constraints: Memory limited; each file must be read sequentially. K up to thousands.

Hint: Use a min-heap of (current_value, file_id). For huge K, use staged merging (merge in batches) or tournament trees to reduce heap size. Consider buffer sizes and async IO.

Real-world relevance: Merging mapper outputs in distributed ETL (e.g., MapReduce reduce phase) or merging sorted partitions for bulk loads.

Target complexity: O(N log K) time where N is total items; O(K) memory for heap + buffers.

Edge cases / Follow-ups: Stable merging, handling of ties, backpressure when writing to slow sink.

12) Find top-k most frequent events in streaming logs (approximate)

Difficulty: Medium

Primary DS: Count-min sketch or Space-Saving algorithm + Heap

Key Concept: Approximate counting, streaming heavy hitters

Scenario: The log stream is unbounded, too large to keep exact counts. You need to continuously report top-k event types (by id) with bounded memory. Provide algorithm and error bounds.

Problem: Design the streaming approximate top-k system and implement core update/query logic.

Input/Output: Stream of event ids → approximate top-k list at query time.

Constraints: Memory limited to a few MB; high throughput (100k/sec).

Hint: Use Space-Saving algorithm (maintain k counters) or Count-Min Sketch with heavy-hitter extraction. Describe error guarantees (overcount bounded by epsilon * total).

Real-world relevance: Real-time monitoring dashboards, detecting popular logs or errors.

Target complexity: O(1) per update (amortized), O(k) query.

Edge cases / Follow-ups: Handling time decay (sliding window top-k), false positives and mitigation.

13) Reconstruct time series with missing days (longest gap)

Difficulty: Medium

Primary DS: HashSet, Sorting, Two pointers

Key Concept: Set membership + scanning

Scenario: Input is a list of record dates (not necessarily continuous). Find the longest consecutive run of missing days between first and last date — i.e., largest gap where no records exist. Data set up to millions of distinct days in multi-year range.

Problem: Return start and end date of largest missing interval and its length.

Input/Output: list of dates → (start_missing_date, end_missing_date, length).

Constraints: Dates may be unsorted. Memory must fit unique day set.

Hint: Sort unique dates and scan adjacent differences, or use HashSet + scanning with day increment (if range small). For large ranges, sorting is safer.

Real-world relevance: Data completeness checks, SLA violations for ingestion pipelines.

Target complexity: O(n log n) if sorting; O(n) additional space.

Edge cases / Follow-ups: If range huge but sparse, propose min/max and choose partitioned scan or external sort.

14) Rebalance partitions by size (approximate greedy)

Difficulty: Medium

Primary DS: Heap, Array

Key Concept: Greedy balancing, bin packing heuristic

Scenario: You have N partitions (files) with sizes; you want to move up to M files to other machines to balance disk usage across R servers. Provide algorithm to suggest moves (approximate optimal).

Problem: Given partition sizes and server capacities, output a plan of moves to reduce max utilization.

Input/Output: arrays of sizes and current server assignments → list of moves (file → server).

Constraints: N large (10k), M limited (few hundred), want minimal total moves.

Hint: Greedy: repeatedly move largest file from most loaded server to least loaded that can accept it; use max-heap and min-heap. Track capacities. Simulated annealing or multi-fit heuristics for better quality.

Real-world relevance: Rebalancing HDFS / S3 partitions, minimizing copy cost.

Target complexity: O(M log R + sort overhead).

Edge cases / Follow-ups: Large file that cannot fit anywhere; minimize network transfer or prefer locality.

15) Find median across multiple sorted shards (online queries)

Difficulty: Medium

Primary DS: Heaps, Binary search over value range

Key Concept: Selection in distributed data, quantile estimation

Scenario: Data is sharded into many sorted files. You need to support online median queries without merging all shards fully. The shards are on different nodes and you can fetch counts for ranges cheaply but not full data.

Problem: Return median value. Describe algorithmic options (e.g., binary search on domain + count queries vs multiway merge limited to middle).

Input/Output: K sorted iterators + ability to count numbers ≤ x per shard → median value.

Constraints: Shards large, network cost nontrivial.

Hint: Use selection by value: binary search candidate value v, request per-shard counts ≤ v, adjust search. Or maintain two heaps reading only needed elements for the middle region.

Real-world relevance: Percentile calculation for metrics across partitions.

Target complexity: O(log V * K * cost_count), where V is value range; or O(N log K) for merge approach with partial scanning.

Edge cases / Follow-ups: Weighted medians, duplicates, even vs odd counts.

16) Detect cycles in job dependency graph and produce recovery plan

Difficulty: Medium

Primary DS: Graph, DFS, Topological sort

Key Concept: Cycle detection, SCCs (strongly connected components)

Scenario: ETL jobs have dependency DAGs but due to config mistakes cycles appear. You must detect cycles, identify minimal set of edges (or jobs) to break to restore acyclic order, and generate alternative ordering after break.

Problem: Given directed graph of job dependencies, return a list of cycles and propose minimal edges to remove (approximate) and a topological order after removal.

Input/Output: adjacency list → cycles + suggested removals + topo order.

Constraints: Nodes up to 100k; edges moderate. Need efficient method.

Hint: Use DFS for cycle detection; find SCCs (Tarjan/Kosaraju) to identify strongly connected components; inside SCC remove edges by heuristics (e.g., least cost edge). Exact minimum feedback edge set is NP-hard; provide approximate heuristics like remove edges causing back edges in DFS.

Real-world relevance: Scheduling ETL tasks, resolving cyclic dependencies in orchestration.

Target complexity: O(V+E) for SCC detection.

Edge cases / Follow-ups: Weighted edges (prefer removing low-cost), automation to propose human review.

17) Rolling aggregate with late arrivals (out-of-order)

Difficulty: Medium

Primary DS: Heap or windowed buffers, HashMap

Key Concept: Windowed aggregation with watermarking, handling out-of-order events

Scenario: Stream aggregator computes sum per minute. Events may arrive up to L seconds late. You must compute final aggregates with lateness handling and emit intermediate results while supporting corrections when late events arrive.

Problem: Design algorithm to maintain rolling per-minute aggregates and support late-event corrections. Implement core structures for buffer and update.

Input/Output: Stream of (timestamp, key, value) → emitted aggregate updates keyed by minute, possibly corrected later.

Constraints: High throughput; lateness L bounded (e.g., 2 minutes). Memory bounded by number of open windows.

Hint: Keep per-window HashMap and update on arrivals. Use watermark to evict windows older than watermark. Use persisted changelogs for idempotency if required. Consider out-of-order buffer with sorted structure keyed by event time to apply corrections in order.

Real-world relevance: Time-series metrics in streaming pipelines (Beam / Flink concepts).

Target complexity: O(1) average update, O(W) memory where W = number of open windows.

Edge cases / Follow-ups: Unbounded late arrivals — switch to eventual consistency or tombstones.

18) Partition a dataset to minimize cross-join cost

Difficulty: Medium

Primary DS: Greedy heuristic, graph modeling

Key Concept: Hash partitioning vs range partitioning tradeoffs, NP-hard partitioning approximations

Scenario: Given several join queries (patterns of join keys between tables) and possible partitioning schemes (hash on columns), choose partitioning for a large fact table to minimize total expected shuffle cost across queries. Provide heuristic algorithm.

Problem: Given m queries and candidate partition keys, suggest partition key(s) and estimate cost.

Input/Output: query patterns and table sizes → partitioning choice + cost estimate.

Constraints: Need heuristics; exact solution may be expensive.

Hint: Model as weighted hypergraph where queries connect columns; try greedy pick of key that covers most join weight; evaluate multi-key compound partition if supported. Simulate cost using known formulas for shuffle given partitioning.

Real-world relevance: Data warehouse partitioning strategy (BigQuery / Spark).

Target complexity: Heuristic O(m log k).

Edge cases / Follow-ups: Multi-tenant queries, skewed keys — consider sampling and skew mitigation.

19) Find longest substring of unique tokens after tokenization

Difficulty: Medium

Primary DS: HashMap / Sliding window

Key Concept: Sliding window on tokenized stream

Scenario: You receive a long text field containing tokens separated by spaces or punctuation. Find the longest contiguous subsequence of tokens with no repetition (unique tokens) to understand vocabulary diversity. Tokens may be large strings.

Problem: Return start index and token count of longest unique token substring.

Input/Output: string → (start_token_index, length).

Constraints: Text length up to tens of thousands tokens. Token equality uses hashing.

Hint: Map token → last index seen; sliding window move left bound to max(last_index + 1, left).

Real-world relevance: Text de-duplication, deduping token sequences or detecting repeated patterns in incoming logs.

Target complexity: O(n) time, O(vocab) space.

Edge cases / Follow-ups: Case sensitivity, normalization, tokenization complexities, huge token vocab requiring approximate methods.

20) Efficiently compute hourly distinct users for 24 hours from 1B events

Difficulty: Medium

Primary DS: HyperLogLog, Streaming sketch

Key Concept: Approximate distinct counting per time bucket

Scenario: Given 1B events with timestamps and user_ids, produce distinct user counts for each of the past 24 hourly windows, memory constrained. Exact counts too expensive.

Problem: Design architecture and algorithm; implement core logic using HyperLogLog (or equivalent). Provide error bounds and merging logic.

Input/Output: Stream → 24 approximate counts (hour buckets).

Constraints: Memory ~ tens of MB; error ≤ 2%.

Hint: Maintain one HLL per hour; use rolling rotate; combine sketches for multi-hour windows. Ensure deterministic hashing for user IDs. Persist sketches periodically.

Real-world relevance: Usage analytics, DAU/MAU approximations.

Target complexity: O(n) updates, O(1) per event, memory O(num_buckets × sketch_size).

Edge cases / Follow-ups: Time zones, late events—handle by write-back corrections or extended retention of sketches.

21) Merge intervals with attribute propagation

Difficulty: Medium

Primary DS: Sorting, Interval merging, HashMap

Key Concept: Merge overlapping intervals and combine metadata

Scenario: Each interval represents a task with a set of tags and weight. When intervals overlap, merge them into disjoint intervals where tags are unioned and weights aggregated.

Problem: Given list of intervals (start, end, tags, weight), output merged disjoint intervals with combined metadata.

Input/Output: list of intervals → merged list.

Constraints: Up to millions of intervals; tags small per interval.

Hint: Sweep line algorithm: create events (start, +tags/weight), (end, −tags/weight), sweep sorted by time maintaining active set and emit segments when active set changes.

Real-world relevance: Resource availability windows and metadata aggregation.

Target complexity: O(n log n) due to sorting, O(active) metadata maintenance.

Edge cases / Follow-ups: Very large number of tag combinations — compress tag representation or use counts.

22) Reorder log lines by event_id with allowed out-of-order window

Difficulty: Medium

Primary DS: Min-heap or ordered buffer

Key Concept: Bounded reordering, watermarking

Scenario: Logs arrive slightly out of order (bounded by W lines). You must output logs in correct event_id order as much as possible — i.e., hold a small buffer for reordering up to W and then flush oldest when safe.

Problem: Implement streaming reordering with buffer size W.

Input/Output: Stream of (event_id, payload) with event_id nearly increasing; output reordered events.

Constraints: W small (e.g., 1000). Throughput high.

Hint: Use min-heap keyed by event_id; keep track of highest emitted id; when heap min ≤ highest_emitted+1, emit sequentially; otherwise if heap size exceeds W, emit smallest to avoid blocking.

Real-world relevance: Reconstructing ordered event streams before ingest into partitioned store.

Target complexity: O(log W) per event.

Edge cases / Follow-ups: Unbounded gaps, duplicate event ids — dedupe strategies.

23) Transform wide table to tall schema (unpivot) efficiently

Difficulty: Medium

Primary DS: Streaming arrays, yield generators

Key Concept: Transformation / streaming unpivoting

Scenario: A table has 200 dynamic metric columns per row. You need to convert each row into up to 200 (metric_key, metric_value) rows for analytics. Must do it streaming with low memory.

Problem: Implement or describe streaming unpivot transformer taking row → multiple rows.

Input/Output: wide row → series of tall rows.

Constraints: Throughput high; avoid materializing entire table.

Hint: Iterate columns per row and write out transformed rows to sink; if many columns are null, skip nulls to reduce IO. Use vectorized IO for batch performance.

Real-world relevance: Preparing normalized data for OLAP ingestion.

Target complexity: O(n × c) where c is columns per row; memory O(batch_size × c) if vectorized.

Edge cases / Follow-ups: Schema evolution (new metrics), dynamic columns — use schema registry.

24) Find duplicate files by content hash in a directory tree

Difficulty: Medium

Primary DS: HashMap (hash → list), streaming file IO

Key Concept: Hashing, chunked hashing to avoid reading whole file when possible

Scenario: Files in directories might be duplicate copies. We need to find groups of duplicate files by content efficiently. Files can be large and many.

Problem: Return groups of file paths that are duplicates. Optimize IO.

Input/Output: list of file paths → groups of duplicates.

Constraints: Total data size large, reading all files costly.

Hint: Multi-pass approach: group by file size, then hash first N bytes, then full hash only for candidates. Use streaming hash (e.g., MD5) chunked to avoid loading full file into memory.

Real-world relevance: Storage deduplication, cleanup.

Target complexity: IO dominated. Aim to minimize full reads.

Edge cases / Follow-ups: Hard link vs copy detection, incremental detection as files change.

25) Range query on time-partitioned data (fast lookup)

Difficulty: Medium

Primary DS: Segment tree or index (B-tree concept)

Key Concept: Range query, precomputed prefix aggregates

Scenario: For an analytics query, you need sum of values in a time range across a huge table partitioned by hour. Each hourly partition has metadata: total count and sum. Design system to answer arbitrary time range sum queries quickly by combining metadata and scanning minimal partitions.

Problem: Given per-hour metadata and ability to scan per-hour partial data, design algorithm to answer range sum with minimal IO.

Input/Output: query(start_time, end_time) → sum.

Constraints: Many small queries; prefer using metadata rather than scanning whole partitions.

Hint: Use prefix sums on hourly metadata to answer full hours instantly; scan partial hours at edges. For sub-hour granularity, use smaller partitioning or secondary index.

Real-world relevance: Dashboard KPIs with time ranges.

Target complexity: O(1) for mostly-aligned ranges + O(scan_edges).

Edge cases / Follow-ups: Time zone normalization, missing partitions.

26) Detect drift in distribution between two samples

Difficulty: Medium

Primary DS: Histograms, KS test approach

Key Concept: Statistical tests, histograms, streaming sketches

Scenario: You have baseline distribution of a numeric feature and a new sample; detect if distribution significantly drifted. Data volumes large; need approximate fast method.

Problem: Implement an algorithm to compare distributions and flag drift. Provide tradeoffs.

Input/Output: baseline sample / streaming sketch + new sample → boolean drift flag + drift score.

Constraints: Memory limited; need fast approximate test.

Hint: Use sketches like quantile summaries or histograms, compute two-sample Kolmogorov–Smirnov approx or chi-squared on binned histograms. For streams, maintain t-digests for quantiles.

Real-world relevance: Data quality monitoring, model input drift detection.

Target complexity: O(n) to build summaries, constant to compare.

Edge cases / Follow-ups: High cardinality categorical features — use PSI (population stability index).

27) Build compact index for time series for fast range retrieval

Difficulty: Medium

Primary DS: B+ tree / Sorted blocks + bloom filters

Key Concept: Block index, bloom filters for existence

Scenario: Design an on-disk index for time series chunks to quickly locate which blocks contain data for a given time range, minimizing scan. Each block holds sorted points.

Problem: Provide data structure and access algorithm. Implement block metadata scanning approach.

Input/Output: index built → query returns list of blocks to scan.

Constraints: Disk IO is expensive. Memory small for index.

Hint: Keep per-block min/max timestamps, optionally Bloom filter for keys (if multi-key), maintain sorted block metadata for binary search. For range queries, binary search to find first block and walk until end.

Real-world relevance: Time-series database design, efficient point/interval queries.

Target complexity: O(log B + number_blocks_to_scan).

Edge cases / Follow-ups: Highly variable block sizes, sparse data in blocks — reorganize compaction.

28) Compute join cardinality estimate from sampled statistics

Difficulty: Medium

Primary DS: Histograms, sketches

Key Concept: Cardinality estimation, independence vs correlation handling

Scenario: Planner needs estimated row count for join A⋈B on key column to plan shuffle size. You have per-table column histograms and distinct counts. Provide method to estimate join size with errors.

Problem: Given histograms and ndv (distinct values), compute estimated join cardinality. Discuss assumptions and corrections.

Input/Output: statistics → estimated join size (and confidence).

Constraints: Histograms coarse; columns may be correlated.

Hint: Under independence, estimate as sum over value v of countA[v] × countB[v]; approximate using histograms by bucket overlap. Use sampling to refine. For skewed distributions, detect heavy hitters separately.

Real-world relevance: Query optimizer cost estimation for planning distributed joins.

Target complexity: O(buckets) operations.

Edge cases / Follow-ups: Multi-column joins, correlations—use multi-dimensional sketches or samples.

29) Streaming dedupe with TTL window

Difficulty: Medium

Primary DS: HashMap with timestamps + time wheel / TTL queue

Key Concept: TTL based state eviction, approximate dedupe

Scenario: De-duplicate incoming events by id for a sliding TTL window (e.g., ignore duplicates within last X minutes). High throughput stream, memory bounded.

Problem: Implement process(event) which returns true if this event is new within TTL else false. Ensure state eviction to control memory.

Input/Output: stream → boolean per event.

Constraints: TTL big (hours), throughput high, many unique ids.

Hint: Keep map id→last_seen_time and an ordered structure (time buckets or queue) to evict old ids when timestamp passes TTL. For high cardinality, use approximate bloom filters with aging.

Real-world relevance: Idempotence in event ingestion, dedupe in CDC.

Target complexity: O(1) per update amortized.

Edge cases / Follow-ups: Memory pressure—switch to LRU eviction, accept probabilistic false positives.

30) Generate a minimal transformation pipeline to compute derived metric

Difficulty: Medium

Primary DS: DAG, topological sort

Key Concept: Dependency resolution, topological ordering

Scenario: You have a set of column derivations where some derived columns depend on others. Given a target derived metric, produce minimal pipeline steps in order and detect cyclic dependencies.

Problem: Input: dependency rules like A = f(B,C), B = g(D) etc. Output: ordered list of computations needed to compute target.

Input/Output: rules list + target → ordered steps or cycle error.

Constraints: Rules up to thousands; need compact plan.

Hint: Build dependency graph, run DFS/Topo sort from target to leaves to extract minimal subgraph; detect cycles. Prefer caching previously computed intermediate results.

Real-world relevance: Building DAGs for ETL/transform engines to compute single artifact.

Target complexity: O(V+E).

Edge cases / Follow-ups: Shared subexpressions optimization, cost-aware ordering.

31) Find connected devices from daily heartbeat graph

Difficulty: Medium

Primary DS: Graph BFS/DFS, Union-Find

Key Concept: Connected components on undirected graph

Scenario: Devices report heartbeats and occasionally connection edges are logged (device A ↔ B). For the day's edges, find connected components (groups of connected devices) for cluster health checks.

Problem: Return list of components (sets of device IDs).

Input/Output: edge list → components list.

Constraints: Up to 1M devices, edges up to a few million. Memory constrained.

Hint: Use Union-Find (disjoint set union) with path compression for efficiency. If graph huge and distributed, run parallel connected components (e.g., label propagation).

Real-world relevance: Network topology health, grouping sensors.

Target complexity: roughly O(E α(V)) time, O(V) space.

Edge cases / Follow-ups: Streaming edges, incremental update of components across days.

32) Compact sparse vector representation and dot product

Difficulty: Medium

Primary DS: HashMap / compressed sparse row (CSR)

Key Concept: Sparse representation, efficient dot product

Scenario: Feature vectors are high dimensional (10M dims) but sparse (<100 nonzero). Need efficient storage and dot product for similarity scoring in model scoring.

Problem: Design storage and implement dot product between two sparse vectors.

Input/Output: map or list representation → scalar dot product.

Constraints: Many vectors stored; operations frequent.

Hint: Use list of (index, value) sorted by index and merge for dot product (two pointers) or hash map for random access if only one small vector. Use CSR for batches.

Real-world relevance: Embedding or feature sparse computations in scoring engines.

Target complexity: O(nnz_a + nnz_b) for two-pointer merge.

Edge cases / Follow-ups: Very skewed sparsity — choose representation accordingly; memory vs CPU tradeoffs.

33) Find the earliest timestamp where cumulative sum crosses threshold across partitions

Difficulty: Medium

Primary DS: Min-heap, two-pointer across sorted lists

Key Concept: K-way time aggregation, prefix sums per partition

Scenario: Multiple partitions of time-ordered event streams each giving counts per timestamp. You want earliest global timestamp where cumulative sum across partitions ≥ T. Partitions are large and stored individually.

Problem: Determine timestamp where global cumulative reaches threshold with minimal IO.

Input/Output: K sorted iterators of (timestamp, delta) → timestamp or null.

Constraints: K large; threshold T moderate.

Hint: Use min-heap to merge per-timestamp and accumulate until threshold reached. If timestamps sparse, can skip partitions ahead using prefix metadata.

Real-world relevance: SLA detection like when total events exceed capacity.

Target complexity: O(M log K) where M processed timestamps.

Edge cases / Follow-ups: Multiple identical timestamps across partitions; late arrivals.

34) Efficient deduplication across time using external sort

Difficulty: Medium

Primary DS: External sorting (merge sort), file IO

Key Concept: External memory algorithm for dedupe on huge dataset

Scenario: Deduplicate records by primary key from a dataset too big to fit in memory (500 GB). Keep the latest record by timestamp. Provide external memory algorithm and I/O plan.

Problem: Output deduped dataset with latest record per key.

Input/Output: unsorted file → deduped output file.

Constraints: Memory small (8 GB), disk/IO available.

Hint: External sort on (key, timestamp) descending so latest per key appears first; then one-pass to drop duplicates. Alternatively, partition by hash into manageable buckets then process each bucket in memory. Choose method based on number of unique keys and IO costs.

Real-world relevance: Large scale ETL deduplication before warehouse load.

Target complexity: O(N log N) disk IO dominated.

Edge cases / Follow-ups: Skewed keys; choose partitioner to balance.

35) Fault-tolerant streaming aggregator with checkpointing

Difficulty: Medium

Primary DS: Checkpointed state (hashmaps persisted), log offsets

Key Concept: Exactly-once semantics, checkpointing & replaying state

Scenario: You maintain stateful aggregation (sum per key). System can crash and replay logs; design aggregator to be fault-tolerant and produce exactly-once results on recovery. Explain data structures and checkpoint strategy.

Problem: Describe design and implement core in-memory state logic; explain checkpoint save/restore.

Input/Output: stream of (offset, key, value); output aggregated results once per watermark.

Constraints: Replay possible; offsets available. Disk for checkpoints.

Hint: Use idempotent writes and offsets metadata, persist last processed offset with state snapshot; on recovery replay messages after last checkpoint. Use write-ahead log or upstream atomic commit to external sink.

Real-world relevance: Reliable streaming ETL with guarantees (Flink/Beam style).

Target complexity: O(1) per event; checkpoint cost periodic.

Edge cases / Follow-ups: Exactly-once across sinks, handling non-commutative ops.

36) Detect heavy hitters with sliding window (exact for heavy ones)

Difficulty: Medium

Primary DS: Count-min sketch for candidates + exact counts for top candidates (heap + hashmap)

Key Concept: Hybrid approximate/exact heavy hitters in slide windows

Scenario: Need exact detection of items whose frequency > φ fraction within last W items. Full exact counts is expensive, but we can approximate to find candidates then verify.

Problem: Design and implement two-phase algorithm returning top heavy hitters exactly.

Input/Output: stream → list of heavy hitters at query time.

Constraints: W large, memory constrained to keep only sketches + verification buffers.

Hint: Use Misra–Gries (Space Saving) to get candidates; maintain exact counter only for candidates. For sliding windows, maintain time buckets and invalidation.

Real-world relevance: Detecting hot keys for dynamic scaling or caching.

Target complexity: O(1) amortized per update.

Edge cases / Follow-ups: Frequency decay and time-weighted heavy hitters.

37) Schema evolution compatibility checker

Difficulty: Medium

Primary DS: Trees (schema AST), graph constraints

Key Concept: Graph walk for compatibility rules (additive vs breaking changes)

Scenario: Given prior schema and candidate schema change (fields added/removed/type changed), determine if change is backwards compatible for consumer apps requiring non-breaking reads. Create checker that enforces rules (e.g., adding optional field OK; changing type breaking).

Problem: Implement compatibility checker returning pass/fail + list of breaking changes.

Input/Output: old_schema, new_schema → compatibility report.

Constraints: Schemas nested, include unions, arrays, optional markers.

Hint: Recursively compare fields; treat added non-nullable fields as breaking; type widening vs narrowing rules. Consider default values and schema registry semantics.

Real-world relevance: CI checks before deploying schema changes for Kafka/Avro/Parquet.

Target complexity: O(size_of_schema).

Edge cases / Follow-ups: Complex union types, logical types, backwards vs forwards compatibility.

38) Compute approximate distinct counts per key with merging

Difficulty: Medium

Primary DS: HyperLogLog per key with mergeable sketches

Key Concept: Mergeable sketches, per-key aggregation

Scenario: For each tenant (key), maintain approximate distinct user count using mergeable sketches that can be merged across workers. Need to implement per-key updating and merging logic.

Problem: Support update(key, user_id) and mergeSketches(sketchA, sketchB).

Input/Output: stream → per key sketches; merge returns sketches that can be queried for approximate counts.

Constraints: Very many keys (100k+), but each sketch small (~1–10 kB).

Hint: Use HyperLogLog with deterministic hash; maintain map key → HLL; for memory, TTL infrequently used keys or use tiered store. Merging is union operation (merge registers).

Real-world relevance: Multi-tenant DAU tracking, per-pipeline cardinalities.

Target complexity: O(1) per update amortized (hash + register update).

Edge cases / Follow-ups: Key explosion, sketch errors for small cardinalities — bias correction needed.

39) Efficient dedupe with approximate bloom + exact confirm (two-stage)

Difficulty: Medium

Primary DS: Bloom filter + HashMap or disk store for confirmations

Key Concept: Probabilistic prefilter + exact confirm fallback

Scenario: Large rerun requires deduping event IDs against large historical dataset stored in cold storage. Minimize expensive cold reads. Use small memory bloom filter to prefilter likely new vs likely old.

Problem: Design two-stage dedupe where bloom filters reduce I/O. Explain false positive handling and update strategy.

Input/Output: stream → deduped events with few cold reads.

Constraints: Historical set extremely large; bloom false positive rate must be managed.

Hint: Build bloom from sampled or periodically built index; on bloom positive, confirm with exact lookup in store; update bloom periodically with new keys. Consider cascading bloom filters for ranges.

Real-world relevance: Fast idempotence checks using caches.

Target complexity: O(1) expected per check; IO minimized.

Edge cases / Follow-ups: Bloom aging for deletions, scaling bloom to billions of keys (partition bloom).

40) Optimize broadcast join vs shuffle join decision

Difficulty: Medium

Primary DS: Cost model, heuristics

Key Concept: Cost estimation, network transfer minimization

Scenario: In distributed query engine, decide whether to broadcast a small table to all workers or perform a shuffle join. Given sizes, memory per worker, and number of workers, decide and outline exact algorithm.

Problem: Return boolean broadcast_ok and explain thresholds.

Input/Output: small_table_size, num_workers, worker_memory, expected partitioning info → decision + rationale.

Constraints: Want low network I/O and avoid spilling.

Hint: Broadcast if small_table_size × num_workers <= acceptable_total_network and small_table_size <= memory_budget_per_worker × fraction. Consider local caching benefits and skew.

Real-world relevance: Query planner decisions in Spark/BigQuery.

Target complexity: O(1) decision.

Edge cases / Follow-ups: Broadcast with compression, multi-join sequences, presence of replication.

Hard — 10 Scenario Questions
41) Build a distributed median estimator with bounded error and merges

Difficulty: Hard

Primary DS: Quantile summaries (t-digest, GK summary), mergeable data structures

Key Concept: Mergeable quantile sketches, error guarantees

Scenario: Multiple workers process partitions and maintain summaries that can be merged to compute global median (50th percentile) with error ε. Design the sketch structure, merging protocol, and recovery when partial merges occur. Implement core merge and query ops.

Problem: Provide algorithm and pseudo-implementation for summary and merge; discuss error bounds and memory.

Input/Output: per-worker streams → global median estimate ±ε.

Constraints: Many workers (100s), limited memory per worker; need deterministic merging.

Hint: Use t-digest for float metrics or GK (Greenwald–Khanna) for deterministic guarantees; ensure merge properties are linear and error remains bounded. Manage compression factor and quantile resolution near tails if needed.

Real-world relevance: Global percentiles in telemetry/metrics systems (p99 latency calculations).

Target complexity: updates O(log K) or amortized constant depending on sketch; merge O(size_sketch).

Edge cases / Follow-ups: Heavy tails, dynamic compression tuning, merging partial results incrementally.

42) Design an exactly-once distributed join pipeline with failures

Difficulty: Hard

Primary DS: State stores (keyed state), transactional logs, two-phase commit patterns

Key Concept: Exactly-once semantics across multiple streams and sinks, idempotency, coordination

Scenario: Two high-throughput streams need to be joined on user_id in a windowed manner and the results written to an external store. System must guarantee exactly-once output in presence of worker crashes and retries. Design the pipeline and core algorithms to ensure correctness with minimal latency.

Problem: Outline full design: state management, checkpointing, barrier alignment, write semantics to sink. Identify what data structures are necessary and how to reconcile duplicates on recovery.

Input/Output: two streams → joined rows committed exactly once to sink.

Constraints: High throughput, low latency desired, sink supports atomic writes only per record (no transactional batch).

Hint: Use distributed snapshotting (Chandy–Lamport style) or checkpoint barriers like Flink; maintain local keyed state and store offsets; for sink, use idempotent upserts keyed by a unique id derived from join (and use external transaction log if available). Use two-phase commit or dedupe in sink via unique write keys. Consider watermark alignment.

Real-world relevance: ETL joins across CDC streams feeding dimensional tables with strict correctness.

Target complexity: design complexity high; per-event processing still O(1) amortized.

Edge cases / Follow-ups: Exactly-once across heterogeneous sinks, wallets, or SIEM systems; state size explosion.

43) Global top-k percentiles across time with sliding windows and expiring data

Difficulty: Hard

Primary DS: Time decayed quantile sketches, hierarchical window summaries

Key Concept: Sliding window quantiles, mergeable data sketches with TTL

Scenario: Need to provide top-k percentiles (e.g., 95th) for each minute rolling over last 24 hours, updated in near real-time. Events arrive out-of-order up to 2 minutes. Memory limited; many windows to maintain. Design system and data structures.

Problem: Detail data structure, window retention policy, update path, query response computation, and how to merge/expire sketches. Implement core merging/expiry semantics.

Input/Output: event stream → real-time percentile per minute.

Constraints: High event rate; need ≤1s query latencies.

Hint: Maintain per-bucket sketches (e.g., per second or per minute) using t-digest; for query, merge last N relevant bucket sketches. Use ring buffers for buckets to expire. For out-of-order events, optionally re-emit corrections by updating bucket and re-computing merging for affected queries. Use hierarchical aggregation to reduce merges for large N.

Real-world relevance: SLA tracking, latency monitoring in observability stacks.

Target complexity: update O(log s) per sketch, query O(k * size_sketch) merges.

Edge cases / Follow-ups: Accuracy vs memory tradeoffs, backpressure for correction storms.

44) Design a scalable system to find near-duplicate documents at petabyte scale

Difficulty: Hard

Primary DS: MinHash, SimHash, LSH (Locality Sensitive Hashing), inverted indices

Key Concept: Approximate similarity search at scale, distributed LSH index

Scenario: Need to detect near-duplicate documents (e.g., similar HTML pages) across a corpus of petabytes with bounded false positive/negative rates and reasonable cost. Design algorithmic pipeline, indexing, sharding, and query execution.

Problem: Provide architecture and algorithm: from shingling, hashing, index building, to query and candidate verification. Discuss distribution, incremental updates, and dedupe thresholds.

Input/Output: large corpus → ability to query or batch find near duplicates above threshold T.

Constraints: Petabyte corpus, cluster resources limited; incremental ingestion required.

Hint: Use shingling → MinHash signatures → LSH buckets (multiple banding) to generate candidate pairs; verify with Jaccard on candidate sets. Partition index by LSH buckets across machines; use reservoir sampling to handle heavy buckets. For incremental updates, append to buckets and periodically reindex heavy buckets.

Real-world relevance: Web crawling dedupe, plagiarism detection, document clustering.

Target complexity: candidate generation sublinear; verification depends on candidate count.

Edge cases / Follow-ups: Near duplicates with paraphrase vs exact copy, very large docs — use sampling strategies.

45) Build a consistent distributed hash table with range queries

Difficulty: Hard

Primary DS: Distributed hash rings (consistent hashing), ordered overlays (Chord vs Skip Graphs)

Key Concept: Distributed indexing with routing and replication, range query extension over DHT

Scenario: Implement a key-value store with distributed nodes supporting point gets, puts, and efficient range scans (scan by key prefix). Must handle node joins/leaves with minimal data movement.

Problem: Describe data structure, node routing logic, replication, and how to support range queries (which DHTs are not naturally good at).

Input/Output: store API (put/get/scan) with distributed nodes.

Constraints: Large cluster (100s nodes), keys hashed but need order for range scans.

Hint: Use consistent hashing for point lookups; for range queries, augment with ordered overlays like Rendezvous hashing + maintaining secondary sorted index (e.g., maintain range shards with contiguous key ranges via partitioner). Alternatively use hybrid design: shard by prefix/range instead of hash. Use replication and consistent rebalancing algorithms. Consider global metadata service for mapping ranges to nodes.

Real-world relevance: Scalable key-value stores requiring ordered scans (time series stores).

Target complexity: Lookups O(1) or O(log N) routing; range scans O(nodes touched + data).

Edge cases / Follow-ups: Hot keys, rebalancing cost, guaranteeing strong consistency vs eventual consistency.

46) Real-time join across many streams with low latency and bounded memory

Difficulty: Hard

Primary DS: Windowed state stores, bloom filter caches, keyed state and eviction policies

Key Concept: Multi-stream join strategies (hash join, stream-hash, semi-join), state management and retention

Scenario: Join K small side streams with one large main stream in real time to enrich events. The small streams are updated frequently; join must be low latency with bounded memory and handle updates to side streams.

Problem: Design mechanism to perform streaming enrichment with correctness, bounded memory, and support for side-table updates (update-in-place semantics). Provide algorithm and data structures.

Input/Output: main stream events → enriched events with side attributes.

Constraints: Side streams small but changing; main stream high throughput. Memory limit ephemeral per worker.

Hint: Load side streams into distributed replicated key-value cache with change feed to update caches. For join, check local cache and do fallback (async lookup) if missing. Use bloom filters per worker to quickly detect non-existence and avoid remote lookups. For correctness with side updates, ensure eventual consistency and versioned attributes if needed. Use TTL and LRU to bound memory.

Real-world relevance: Real-time enrichment (user profiles) in ad tech or personalization.

Target complexity: O(1) average lookup; memory O(size_of_cached_keys).

Edge cases / Follow-ups: Strong consistency requirements -> use transactional reads, impact on latency.

47) Design a multi-tenant metric store with fast rollups & retention policies

Difficulty: Hard

Primary DS: Time-series chunking, downsampling sketches, tiered storage indices

Key Concept: Tiered storage (hot/warm/cold), rollup aggregation, retention compaction

Scenario: Multi-tenant monitoring system ingesting counters/gauges at high rate. Need to retain raw 1s samples for 1 day, 1m rollups for 30 days, and 1h rollups for 3 years. Provide data structures, compaction algorithms, and queries for range + rollup retrieval. Also discuss tenant isolation and cost control.

Problem: Design and justify storage layout, rollup pipelines, indexing, and query resolution procedure.

Input/Output: time series writes → compacted storage + query API returning best available resolution.

Constraints: High cardinality per tenant, tight cost budget, cold storage slower.

Hint: Ingest into hot buffer (chunked time blocks), periodically compact to rollups via mapreduce or streaming aggregator producing lower-resolution aggregates, store chunk metadata with time ranges and resolution. For queries, choose highest resolution available covering range and combine blocks. Use tenant quotas and eviction.

Real-world relevance: Prometheus/Influx-style metric storage at scale.

Target complexity: Write O(1) amortized; compaction O(N) batch.

Edge cases / Follow-ups: High-cardinality spikes, downsample accuracy (max vs avg), retention deletion.

48) Federated search ranking with partial signals from shards

Difficulty: Hard

Primary DS: Ranking models, top-k merge algorithms (distributed top-k), approximate sketches

Key Concept: Federated aggregation of scores, normalization and re-ranking, partial recall

Scenario: Search across multiple shards returns top-m candidates per shard with local scoring. Need global top-k results with minimal cross-shard communication and consistent ranking. Scores are not directly comparable across shards. Design algorithm to merge and re-rank efficiently with minimal data transfer.

Problem: Provide merge algorithm, score normalization technique, and fallback for ties/correction. Discuss tradeoffs between recall and latency.

Input/Output: shard responses (doc_id, local_score) → global top-k ranked docs (doc_id, global_score).

Constraints: Many shards, low latency (100ms).

Hint: Use two-phase retrieval: first retrieve top-m from each shard using approximate local scoring; collect candidate set and compute global re-rank by fetching feature vectors or re-running centralized model on candidates. Normalize scores using calibration (e.g., learn shard bias offsets). Use priority queue merging for initial candidate union.

Real-world relevance: Search engines, recommendation retrieval.

Target complexity: O(S × m log k) initial, re-rank on |candidates|.

Edge cases / Follow-ups: Shard scoring drift, stale shard models, fairness across tenants.

49) Multi-stage pipeline optimization: choose materialization points

Difficulty: Hard

Primary DS: DAG, dynamic programming cost model

Key Concept: Tradeoff between recomputation and storage (materialize vs compute), DP/planner for execution cost

Scenario: A multi-stage transformation DAG computes many downstream metrics. You can choose to materialize (persist) results between stages to speed repeated downstream queries at storage cost. Given query frequency and compute cost per node, decide which nodes to materialize under storage budget to minimize total expected cost.

Problem: Formulate optimization and produce approximate algorithm to select nodes to materialize.

Input/Output: DAG with compute_cost(node), size(node), query_frequency(node) → set of nodes to materialize.

Constraints: Storage budget S. DAG up to thousands nodes.

Hint: This maps to knapsack over DAG with dependencies; dynamic programming with memoization if DAG small; greedy heuristics using benefit/cost ratio (expected saved compute per storage unit) often practical. Consider transitive savings when materializing upstream nodes that help multiple downstreams.

Real-world relevance: Data platform deciding where to persist intermediate tables (cache vs compute).

Target complexity: NP-hard in general; heuristics O(V log V) for greedy.

Edge cases / Follow-ups: Materialization invalidation on upstream changes; versioning cost.

50) Build a scalable lineage tracker that finds root causes

Difficulty: Hard

Primary DS: Directed acyclic graph with metadata, BFS/DFS, time series analytics

Key Concept: Causality graph traversal, time correlation, anomaly propagation model

Scenario: Given a large set of ETL jobs and data artifacts with dependencies, failures occur periodically. You must find the likely root cause job(s) given observed failure symptoms (e.g., missing records downstream) across multiple pipelines and time. Design data structures to ingest job runs, errors, metrics and an algorithm to suggest root causes ranked by likelihood.

Problem: Specify how to store lineage, how to query for probable root causes, and implement traversal/score aggregation logic. Include heuristics combining dependency distance, temporal precedence, and error severity.

Input/Output: lineage graph + run histories + failure signals → ranked candidate root causes.

Constraints: Graph large (100k nodes), many historical runs. Need near-real-time inference.

Hint: Build adjacency lists for dependencies, maintain time-indexed run logs. On symptom, traverse upstream graph up to depth D and collect nodes with abnormal run statuses in relevant time window. Score candidates by (1) proximity upstream, (2) frequency of failure in window, (3) correlation with observed metrics. Use Bayesian scoring or weighted heuristics for ranking. Use caching and incremental indexes for speed.

Real-world relevance: Incident analysis and alert triaging in data platforms.

Target complexity: O(nodes_searched + edges) but limited by depth D; use pruning heuristics.