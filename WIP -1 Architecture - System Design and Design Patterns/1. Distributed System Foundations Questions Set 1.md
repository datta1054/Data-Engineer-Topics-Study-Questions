# üìë Index

- [TL;DR ‚Äî citations for core claims](#tldr--citations-for-core-claims)
- [EASY (3) ‚Äî quick wins: clear, behavioral + short design](#easy-3--quick-wins-clear-behavioral--short-design)
   - [Easy 1 ‚Äî CAP / PACELC (system design concept)](#easy-1--cap--pacelc-system-design-concept)
   - [Easy 2 ‚Äî Read-Your-Own-Writes & Session Guarantees](#easy-2--read-your-own-writes--session-guarantees)
   - [Easy 3 ‚Äî Idempotency for APIs](#easy-3--idempotency-for-apis)
- [MEDIUM / INTERMEDIATE (5) ‚Äî typical whiteboard + followups](#medium--intermediate-5--typical-whiteboard--followups)
   - [Medium 1 ‚Äî Partitioning / Sharding design (practical)](#medium-1--partitioning--sharding-design-practical)
   - [Medium 2 ‚Äî Distributed transactions: When to use 2PC vs Sagas](#medium-2--distributed-transactions-when-to-use-2pc-vs-sagas)
   - [Medium 3 ‚Äî Idempotency + Exactly-once in streaming pipelines (Kafka)](#medium-3--idempotency--exactly-once-in-streaming-pipelines-kafka)
   - [Medium 4 ‚Äî Spark shuffle & task-skew mitigation](#medium-4--spark-shuffle--task-skew-mitigation)
   - [Medium 5 ‚Äî Kafka ordering + partitioning tradeoffs](#medium-5--kafka-ordering--partitioning-tradeoffs)
- [HARD (2) ‚Äî deep whiteboard + production correctness](#hard-2--deep-whiteboard--production-correctness)
   - [Hard 1 ‚Äî Architecting end-to-end exactly-once across heterogeneous sinks](#hard-1--architecting-end-to-end-exactly-once-across-heterogeneous-sinks)
   - [Hard 2 ‚Äî Build a multi-region low-latency event store with ordering & failure recovery](#hard-2--build-a-multi-region-low-latency-event-store-with-ordering--failure-recovery)
- [Final interview prep tips (how to deliver these answers)](#final-interview-prep-tips-how-to-deliver-these-answers)


# TL;DR ‚Äî citations for core claims

* PACELC (Abadi): formalizes CAP extension ‚Üí tradeoffs P:(A vs C) else:(L vs C). ([UMD Computer Science][1])
* Spark scheduling, shuffle, speculative execution ‚Äî official docs (tasks write map outputs to local disk; driver schedules; speculative duplicates). ([Apache Spark][2])
* Kafka ordering/offsets and transaction/EOS mechanisms ‚Äî Kafka docs & Confluent blog (ordering per partition, transactional producers & EOS). ([Apache Kafka][3])
* Two-Phase Commit is blocking; practical alternatives (sagas, consensus) and failure modes are well-documented. ([Alibaba Cloud][4])

---

# EASY (3) ‚Äî quick wins: clear, behavioral + short design

## Easy 1 ‚Äî CAP / PACELC (system design concept)

**How they ask:**
‚ÄúQuick ‚Äî explain CAP theorem and PACELC. Give a 2‚Äì3 sentence definition, then tell me which you would pick for a geo-replicated analytical store and why.‚Äù

**Interviewer is evaluating:**
concise correctness, ability to map theory ‚Üí system choice, knowledge of PACELC nuance.

**Approach (what to say, order):**

1. One-line CAP (and limitation).
2. One-line PACELC and why it matters.
3. Pick for geo-replicated analytics and justify with tradeoffs + operational knobs.

**Model answer (speak this):**

1. *CAP:* In presence of a network partition you must choose between **Consistency** (C) and **Availability** (A); Partition tolerance (P) is a must for distributed systems, so systems end up trading C vs A. (one sentence).
2. *PACELC:* Adds that **even when there‚Äôs no partition (Else)** you trade **Latency** vs **Consistency** ‚Äî so designs pick P:(A|C) and E:(L|C). This matters because most production tradeoffs happen during normal operation, not just partitions. ([UMD Computer Science][1])
3. *Choice for geo-replicated analytics:* pick **AP with eventual consistency** for ingestion/OLAP (prioritize availability and low write latency across regions), but provide **on-demand consistent reads** by routing critical analytic queries to a central strongly-consistent view (materialized or via synchronous aggregation) when accuracy is required. Operational knobs: tunable consistency / read-repair / periodic compaction / materialized consistent snapshots.
   **Why this answer works:** fast ingest at global scale is primary for analytics; occasional stronger consistency achieved via controlled paths.

**Follow-ups to prep:** talk about RPO/RTO, data freshness windows, how OLAP can often tolerate eventual convergence.

---

## Easy 2 ‚Äî Read-Your-Own-Writes & Session Guarantees

**How they ask:**
‚ÄúExplain read-your-own-writes and give two practical ways to implement it in a multi-region service.‚Äù

**Interviewer is evaluating:** correctness, practical ops.

**Approach:** define briefly ‚Üí two engineering patterns ‚Üí tradeoffs.

**Model answer:**

* Definition: *Read-your-own-writes* guarantees that after a client writes, subsequent reads by the same client will reflect that write (session guarantee).
* Implementation patterns:

  1. **Session stickiness**: route the client to the same replica (or region) for reads until session expires ‚Äî cheap, but can affect load/availability.
  2. **Client token + read routing**: client gets a write token (or uses a versioned read), and reads are routed or merged from replicas that have applied that token; or use **read-after-write cache** on the client to show immediate results until replicas converge.
* Tradeoffs: stickiness hurts failover; token + merging needs metadata and increases read latency.

---

## Easy 3 ‚Äî Idempotency for APIs

**How they ask:**
‚ÄúWhy do we need idempotency in distributed ingestion? Show a concrete idempotency design for an event ingestion API.‚Äù

**Interviewer is evaluating:** operational sense; simple coding/DB practice.

**Approach:** explain why ‚Üí propose idempotency key pattern ‚Üí show DB schema & flows.

**Model answer:**

* Why: network retries and producer crashes will create duplicates; idempotency prevents duplicate side-effects.
* Design: client sends `Idempotency-Key: UUID` with each logical event. Server stores in a **dedupe table** keyed by `idempotency_key` ‚Üí columns: `(idempotency_key, request_hash, created_at, result_reference, ttl)`. On receiving: do `INSERT ... IF NOT EXISTS` (atomic); if inserted ‚Üí process and write `result_reference`; else fetch stored result and return. Use DB conditional upsert (e.g., Postgres `INSERT ... ON CONFLICT DO NOTHING` + read). Evict entries after retention TTL.
* Notes: keep TTL to bound storage; consider compaction; use strong durable store for the dedupe table.

**Ops tip:** show a one-line SQL upsert example for Postgres (optional in interview).

---

# MEDIUM / INTERMEDIATE (5) ‚Äî typical whiteboard + followups

## Medium 1 ‚Äî Partitioning / Sharding design (practical)

**Question phrasing (how they ask):**
‚ÄúDesign the partitioning/sharding strategy for a time-series metrics store for many customers. They need efficient writes (ingest millions per second), efficient time-range reads, and per-customer retention. Show sharding keys and how to avoid hotspots.‚Äù

**What they evaluate:** choice of shard key, handling hotspots, rebalancing, retention, practical tradeoffs.

**Approach (structure to present):**

1. Requirements & scale assumptions.
2. Sharding strategy choice + diagram.
3. Hotspot mitigation.
4. Rebalancing & retention plan.
5. Monitoring & metrics.

**Detailed solution (say this on whiteboard):**

1. **Assumptions:** e.g., 10M events/s, 100K customers, retention 90 days, need time-range queries per customer and global rollups. (state numbers ‚Äî interviewer may supply or ask you to assume).
2. **Shard key:** use composite key: `shard_id = hash(customer_id) % N_shards` + `time_bucket` (hourly/day). Store events in underlying partitions `partition = (shard_id, time_bucket)`. This gives horizontal distribution across customers and time locality for range scans (time_bucket helps TTL/retention).

   * Diagram (ASCII):

     ```
     customer_id -> hash -> shard (0..N-1)
     shard + time_bucket -> partition on disk (shard0-2025-10-26)
     ```
3. **Avoid hotspots:** if a very hot customer exists, use **salting**: append `salt` (0..k-1) to customer hash to create k sub-shards for that customer. On reads aggregate across salts (map-reduce style). Or detect hot keys and dynamically increase their salt factor.
4. **Rebalancing:** use range splits / consistent hashing to add shards with minimal data move (choose hash ring / consistent hashing). For time-buckets, retention deletes whole buckets simplifying cleanup. When rebalancing, use background data migration and a routing layer (lookup service) to map keys to new shards.
5. **Replication & Availability:** replicate each shard to 3 nodes across AZs; leader handles writes; followers serve reads for less-strong reads. Use tunable quorum for strong reads when necessary.
6. **Monitoring:** per-shard QPS, bytes/sec, per-customer QPS, size per shard, GC/IO latency. Set alarms for >X% skew.

**Tradeoffs / why:** composite key balances distribution and range reads; salting trades read complexity for write distribution.

**Followups to prep:** how to implement lookup service, impact on joins across shards, multi-shard transactions (avoid where possible).

*(You can mention PACELC briefly here: choosing AP for availability during partition, while accepting some eventual consistency for metrics.)* ([UMD Computer Science][1])

---

## Medium 2 ‚Äî Distributed transactions: When to use 2PC vs Sagas

**How they ask:**
‚ÄúYou have a microservice architecture: OrderService, InventoryService, BillingService. A single logical transaction must update all three services. Compare Two-Phase Commit vs Sagas. Which do you pick and why? Show the sequence diagrams for both.‚Äù

**What they evaluate:** correctness of 2PC and Saga tradeoffs, ability to recommend for production.

**Approach:** define both, show sequence diagrams (prepare/commit vs orchestration/compensation), pros/cons, and recommended choice.

**Detailed solution:**

1. **2PC (classical):** coordinator ‚Üí prepare all participants ‚Üí if all YES ‚Üí commit; else abort. Sequence (short ASCII):

   ```
   Coordinator -> Participant: PREPARE
   Participant -> Coordinator: YES/NO (log prepared)
   Coordinator -> All: COMMIT / ABORT
   ```

   * **Pros:** strong atomicity (all or none).
   * **Cons:** blocking ‚Äî if coordinator dies after prepare, participants wait (locks); high latency (two round trips + durable logs); scaling and availability issues for many microservices. ([Alibaba Cloud][4])

2. **Saga (recommended for microservices):** decompose into sequence of local transactions, each step has a compensating action.

   * **Orchestration pattern:** a Saga orchestrator invokes services sequentially: `Order -> Inventory -> Billing`. On failure at Billing, orchestrator triggers compensating actions: `Inventory.Revert`, `Order.Cancel`.
   * **Pros:** non-blocking, fits microservices, simpler failure domain, better availability.
   * **Cons:** eventual consistency, requires writing correct compensating logic, can be complex for long-running flows and failures during compensation.

3. **Recommendation:** Prefer **Sagas** for product microservices (Order/Inventory/Billing) because they enable higher availability and avoid global blocking. Use 2PC only if strict cross-service ACID is non-negotiable and the number of participants is small and service latency/downtime cannot be tolerated.

4. **Operational considerations:** track saga state, idempotency of steps, reconciliation jobs for orphaned sagas, tracing for debugging.

**Followups to prepare:** talk about corner cases (partial failures during compensation), transactional outbox pattern to reliably publish events.

---

## Medium 3 ‚Äî Idempotency + Exactly-once in streaming pipelines (Kafka)

**How they ask:**
‚ÄúWe use Kafka as the ingestion layer and a stateful stream processor writes results to a database and emits to an output topic. How do you ensure **exactly-once** processing from input topic ‚Üí transformation ‚Üí output topic?‚Äù

**What they evaluate:** knowledge of Kafka transactions, offsets, and application architecture to achieve EOS; practical tradeoffs.

**Approach:** outline common patterns and the Kafka transactional solution.

**Detailed solution (what to say):**

1. **Problem:** consumer reads input, processes, writes results to DB and emits to output topic. Crash between DB write and offset commit can cause duplicate processing or data loss. Need atomic read-process-write + offset commit.
2. **Options:**

   * **At-least-once + idempotent sinks** (idempotency keys in DB): simpler, often used.
   * **Outbox + CDC** (transactional writes + change data capture) ‚Äî write result + outbox event atomically to DB; separate process reads outbox and writes to Kafka ‚Äî avoids distributed transaction across DB and Kafka.
   * **Kafka Transactions (EOS):** use transactional producer + consumer to atomically write outputs and commit consumer offsets in a transaction. Steps:

     * Use a transactional producer with a `transactional.id`. On restart, the transaction coordinator fences previous producers.
     * Read input, begin transaction, produce output messages, send offsets to transaction (so offsets are part of the transaction), commit transaction. When committed, both messages and offsets are visible atomically; on failure they are aborted. This ensures EOS for read-process-write cycles. ([Confluent Documentation][5])
3. **Practical caveats:** Kafka EOS requires broker and client configuration, increases complexity, and transactions are not free ‚Äî consider when required. Outbox+CDC approach is more DB-centric and simpler if DB supports transactional outbox.

**Example snippet (logical):**

* `producer.initTransactions()`
* `consumer.poll()` ‚Üí get records
* `producer.beginTransaction()`
* process & produce to output topic
* `producer.sendOffsetsToTransaction(offsets, consumerGroup)`
* `producer.commitTransaction()`

**Monitoring:** transaction abort rates, transaction coordinator load, producer fencing issues.

---

## Medium 4 ‚Äî Spark shuffle & task-skew mitigation

**How they ask:**
‚ÄúIn a Spark job you see a long tail: >99th percentile tasks run 3√ó slower than median and job misses SLA. How do you investigate and fix it? Explain scheduling, shuffle internals, and fixes.‚Äù

**What they evaluate:** low-level Spark engine knowledge, practical debugging and optimization.

**Approach:** quick root cause checklist ‚Üí deep dive on shuffle/spec exec internals ‚Üí fixes.

**Detailed solution (speak this):**

1. **Investigation checklist (what to check first):**

   * Task logs for GC / OOM / serialization issues.
   * Data skew: check input partition sizes and map output sizes.
   * Data locality: are tasks scheduled far from data?
   * Disk/network hotspots: slow nodes, disk IO wait.
2. **How Spark works (short):** Driver builds DAG ‚Üí stages bounded by shuffle ‚Üí DAGScheduler creates tasks per partition ‚Üí TaskScheduler assigns tasks to executors. Shuffle: map tasks write map outputs to local disk, reducers fetch map outputs across executors; if executor is removed, map output must be recomputed. Speculative execution can re-run stragglers (Spark can launch duplicate tasks). ([Apache Spark][2])
3. **Potential root causes:**

   * **Skewed partition** (big partition ‚Üí long reduce).
   * **Straggler executors** (GC, slow disk, network).
   * **Shuffle file unavailability** (executor died) causing recompute.
4. **Fixes (practical):**

   * **Fix skew:** salting keys or pre-aggregate on map side; use `repartition`/`coalesce` carefully; use skew-join optimization (Adaptive Query Execution) or switch to broadcast join if one side is small.
   * **Increase parallelism**: increase `spark.sql.shuffle.partitions` or `spark.default.parallelism` for wide stages to spread load.
   * **Speculative execution:** enable `spark.speculation` for the job, tune `spark.speculation.quantile` and `spark.speculation.multiplier`, but use carefully ‚Äî duplicates cost resources. Databricks docs explain speculative exec behavior. ([kb.databricks.com][6])
   * **Tune GC & executors**: choose right executor memory, off-heap, use Kryo serializer, reduce object churn.
   * **Push-based shuffle / shuffle service**: ensure external shuffle service is enabled if using dynamic allocation so shuffle files persist beyond executor lifecycle, avoiding recomputation.
5. **Synthesis:** start with data skew detection and fix; enable or tune speculative exec only after understanding cost; monitor shuffle read/write metrics and executor GC.

**Followups:** be ready to show exact configs you‚Äôd change and reason about cost vs benefit.

---

## Medium 5 ‚Äî Kafka ordering + partitioning tradeoffs

**How they ask:**
‚ÄúHow do you guarantee ordering in Kafka for events that need ordering per user, while still achieving throughput? Show partitioning design and how you handle hot users.‚Äù

**What they evaluate:** Kafka partitioning design, tradeoff between ordering and parallelism, mitigation for hotspots.

**Approach:** explain partition semantics ‚Üí design keyed partitioning ‚Üí hot key mitigation.

**Detailed solution:**

1. **Kafka guarantees:** ordering is guaranteed **only within a partition**. So to preserve ordering for a key (user), send all user events to the **same partition** (use message key = user_id). ([Apache Kafka][3])
2. **Design:** partition by `hash(user_id) % P`. Consumers in a consumer group are assigned partitions ‚Äî so per-user ordering => single consumer handles that user‚Äôs stream.
3. **Hot users:** a heavy user (hot key) will cause a partition hotspot. Mitigation strategies:

   * **Key sharding (salting):** prefix `user_id` with a salt (0..k-1) for high throughput; consumers when reading must maintain ordering per salted subkey **or** post-process to reconcile order across salts if absolute order is required (expensive).
   * **Use a dedicated topic or partition range** for known hot users and scale consumers for that topic.
   * **Design system to relax ordering** when possible (e.g., make operations commutative or idempotent).
4. **Throughput vs ordering tradeoff:** more partitions ‚áí higher throughput but each partition can be assigned to only one consumer at a time for ordering, reducing per-key parallelism.

**Ops notes:** monitor per-partition throughput & consumer lag; set `acks` and `min.insync.replicas` appropriately for durability vs write latency.

---

# HARD (2) ‚Äî deep whiteboard + production correctness

## Hard 1 ‚Äî Architecting end-to-end exactly-once across heterogeneous sinks

**How they ask:**
‚ÄúDesign an ingestion pipeline that reads events from Kafka, enriches them in Spark Structured Streaming, writes results to PostgreSQL (normalized tables) and to an output Kafka topic **exactly once** despite failures. Give sequence diagrams, identify failure points, and show how you‚Äôll prove correctness.‚Äù

**What they evaluate:** deep system-level knowledge, ability to combine Spark and Kafka semantics, practical alternatives (outbox, transactions), and correctness proofs/argument.

**Approach (structure):**

1. Clarify assumptions & scale.
2. Show candidate designs (favored approach + alternatives).
3. For chosen design show sequence, failure cases, and correctness reasoning.
4. Operational and monitoring plan.

**Solution (detailed, stepwise ‚Äî what to say):**

1. **Assumptions:** streaming throughput moderate (hundreds K/s), PostgreSQL must reflect processed events without duplicates, external downstream needs topic with exactly-once semantics. PostgreSQL does not support participating in Kafka transaction atomic commit out of the box.

2. **Candidate options (pros/cons):**

   * **Option A ‚Äî Kafka EOS with Spark Structured Streaming integration:** Spark can use Kafka as source/sink and support *exactly-once* writes to Kafka topic via transactions and commit offsets atomically (Spark Structured Streaming + Kafka sink with `outputMode=append` and `checkpointLocation`). But writing simultaneously to an external DB breaks atomicity.
   * **Option B ‚Äî Transactional Outbox pattern (recommended):** write application state and an **outbox** row **in the same DB transaction**; a CDC process (e.g., Debezium) picks outbox changes and writes to Kafka. This makes DB and Kafka consistent because DB commit point becomes the source of truth. Downstream consumers get the Kafka message and can be idempotent. This avoids distributed 2PC across DB+Kafka. (Recommended for relational sinks.)
   * **Option C ‚Äî Two-phase commit across Kafka & Postgres:** theoretically possible if Postgres + Kafka were participants in a distributed transaction coordinator, but impractical (no native XA integration and 2PC is blocking and complex). Use only when absolutely required. ([Alibaba Cloud][4])

3. **Chosen design (Outbox + CDC) ‚Äî sequence diagram (high level):**

   ```
   Producer -> Kafka (ingest)  ---> Spark Structured Streaming (enrich)
                          |
                          v
                    write to Postgres transaction:
                      BEGIN
                        INSERT INTO results_table (...);
                        INSERT INTO outbox (payload, topic, key, status);
                      COMMIT
   CDC reads outbox rows -> writes to Kafka output topic (exactly once using idempotent writes or transactional writes)
   ```

   * **Why it‚Äôs correct:** DB commit is the single source of truth for both state and outbox message. CDC ensures the message is produced once (DB ensures exactly-once for outbox row write). CDC + Kafka producer must be configured for idempotent/transactional writes or implement dedupe at consumer to reach end-to-end EOS.

4. **Failure modes & correctness proofs:**

   * **Crash after DB commit but CDC not yet written to Kafka:** message eventually sent by CDC when it resumes ‚Äî no duplication because CDC writes idempotently keyed by outbox id.
   * **CDC writes to Kafka but fails before marking outbox as sent:** ensure CDC marks outbox as sent in DB in same transactional step or uses local idempotency tokens so retries do not duplicate.
   * **Spark retries:** Spark structured streaming should write to DB idempotently (use primary key or idempotency key) and run with checkpointing to avoid duplicate processing on restart.

5. **Operational points:** retention & cleanup of outbox rows after confirmation; monitoring CDC lag; ensure Kafka producer has `enable.idempotence=true` and use transactions if multiple partitions need atomicity.

6. **Proof sketch:** argue invariants ‚Äî (a) DB commit is atomic for state+outbox; (b) CDC reads committed outbox entries exactly once (via transactional CDC or idempotent writes). So end-to-end there is at most one effect in DB and exactly one message on Kafka (bounded by CDC reliability and idempotence).

**Citations & references:** Kafka transactions provide read-process-write EOS semantics when all writes stay within Kafka, but combining external DB requires outbox/CDC or distributed commit alternatives. ([Confluent Documentation][5])

**Followups interviewers may ask:** show exact SQL schema for outbox, how Debezium handles transactional guarantees, how to purge outbox, and how to prove no duplicates under replays.

---

## Hard 2 ‚Äî Build a multi-region low-latency event store with ordering & failure recovery

**How they ask:**
‚ÄúDesign a multi-region event store that: (a) preserves causal ordering per entity, (b) serves low-latency reads in each region, (c) tolerates region partitions and recovers without data loss. Draw architecture, explain leader election across regions, and give conflict resolution rules.‚Äù

**What they evaluate:** system design at scale, causal consistency, conflict resolution, leader election, practical tradeoffs (PACELC), and operational complexity.

**Approach (structure to present):**

1. Clarify constraints (is cross-region strong consistency required?)
2. Propose architecture with reasoned tradeoffs (likely compromise: per-entity primary in one region with async replication + causal guarantees via dependency metadata).
3. Show leader election and failure recovery design.
4. Conflict resolution and correctness argument.

**Detailed solution (what to say):**

1. **Assumptions:** strong global linearizability is **not** required for all operations (that would force high global latency). We must preserve **causal ordering** per entity and guarantee **no data loss** across partitions.

2. **High level architecture (recommended):**

   * **Entity-sharded primary** model: Each entity (e.g., user, account) has a **primary region** (can be assigned by consistent hashing or placement policy). All writes for that entity go to its primary region (leader). That leader synchronously replicates to local replicas in the region (for local HA) and **asynchronously replicates** to other regions for low-latency reads globally. This is P: choose **C** for that entity's primary during partition but **A** elsewhere ‚Äî maps to PACELC: P:(C over A) for that entity if you block writes in non-leader region, else accept weaker guarantees. ([UMD Computer Science][1])
   * **Causal metadata:** attach per-write dependency vector (logical time or client session tokens). For lightweight causal, use **Lamport clocks + per-entity versions** or **session vector clocks**. For full causal, vector clocks can be expensive; use **client session tokens** to preserve client order.
   * **Read model:** local reads served from the nearest replica (possibly stale). For read-your-own-writes, route client to leader or use session tokens to ensure reads see required dependencies (staleness bounds).

3. **Leader election & failure recovery:**

   * Use **Raft/Paxos** per-shard to elect local leader among replicas in a region for low latency; keep global placement metadata (mapping entity ‚Üí primary region) in a small control plane replicated across regions via consensus (e.g., a global config service). On region failure, reassign primaries for affected entities to other regions via global consensus and run recovery to catch up writes using replication logs. Heartbeats and election timeouts tuned for inter-region latencies. (Leader election patterns and heartbeat tuning are standard; tune timeouts to avoid split-brain.) ([MIT CSAIL][7])

4. **Conflict resolution:** because writes route to per-entity primary, conflicts are rare. For active-active writes (if allowed), adopt CRDTs for commutative operations (counters, sets) or use **last-write-wins (LWW)** with causal timestamps + application-aware conflict resolution for more complex state. If strict global ordering needed, block writes in other regions (synchronous cross-region consensus) at expense of latency.

5. **Failure & partition handling:**

   * If a region partitions and becomes isolated, you can choose:

     * **Block writes to entities whose primary is in other regions** (favor consistency) or
     * **Allow writes locally and accept divergence**, then reconcile (favor availability). This is the classic CAP choice; choose based on product needs and document expected reconciliation semantics. (PACELC tradeoffs apply here explicitly.) ([UMD Computer Science][1])

6. **Operational considerations:** placement rebalancing, metadata scale, monitoring cross-region replication lag, and tension between latency and consistency ‚Äî prepare SLOs that reflect these tradeoffs.

**Why this design:** provides local low-latency reads and preserves causal ordering via leader per entity and dependency metadata while offering a clear operational path for failover and reconciliation.

**Followups to prepare:** dive into how to migrate primaries without losing writes, how to handle split-brain when mapping switch occurs simultaneously in two control planes, and how to compress causal metadata for scale.

---

# Final interview prep tips (how to deliver these answers)

1. **Structure answers:** define ‚Üí constraints/assumptions ‚Üí proposal ‚Üí tradeoffs ‚Üí ops & SLOs ‚Üí example numbers. Interviewers want clear tradeoff reasoning above cleverness.
2. **Use the whiteboard:** draw simple diagrams (shard ‚Üí replicas; timeline showing offsets; 2PC prepare/commit). Two quick diagrams often beat long monologue.
3. **Quantify when possible:** give assumed QPS, partition counts, or expected latencies ‚Äî it shows operational mindset.
4. **Finish with operation plan:** monitoring metrics, alert rules, rollback strategy, and migration/rebalancing notes. Product teams care about runbooks.

---


[1]: https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf?utm_source=chatgpt.com "Consistency Tradeoffs in Modern Distributed Database ..."
[2]: https://spark.apache.org/docs/latest/job-scheduling.html?utm_source=chatgpt.com "Job Scheduling - Spark 4.0.1 Documentation"
[3]: https://kafka.apache.org/documentation/?utm_source=chatgpt.com "Kafka 4.1 Documentation"
[4]: https://www.alibabacloud.com/blog/tech-insights---two-phase-commit-protocol-for-distributed-transactions_597326?utm_source=chatgpt.com "Two-phase Commit Protocol for Distributed Transactions"
[5]: https://docs.confluent.io/kafka/design/delivery-semantics.html?utm_source=chatgpt.com "Message Delivery Guarantees for Apache Kafka"
[6]: https://kb.databricks.com/scala/understanding-speculative-execution?utm_source=chatgpt.com "Understanding speculative execution"
[7]: https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf?utm_source=chatgpt.com "Perspectives on the CAP Theorem"
