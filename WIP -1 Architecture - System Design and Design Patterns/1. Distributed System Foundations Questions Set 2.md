# üìë Index

- [Sources used for the questions / facts](#sources-used-for-the-questions--facts)
- [EASY (2)](#easy-2)
   - [Easy 1 ‚Äî Spark inferSchema at scale (from Visa interview reports)](#easy-1--spark-inferschema-at-scale-from-visa-interview-reports)
   - [Easy 2 ‚Äî Consistency between Kafka and downstream data lake (Goldman report)](#easy-2--consistency-between-kafka-and-downstream-data-lake-goldman-report)
- [MEDIUM (4)](#medium-4)
   - [Medium 1 ‚Äî Spark OOM troubleshooting (Goldman)](#medium-1--spark-oom-troubleshooting-goldman)
   - [Medium 2 ‚Äî Kafka consumer lag reduction without losing events (Goldman)](#medium-2--kafka-consumer-lag-reduction-without-losing-events-goldman)
   - [Medium 3 ‚Äî Handling schema evolution for JSON in Kafka (reported)](#medium-3--handling-schema-evolution-for-json-in-kafka-reported)
   - [Medium 4 ‚Äî Data skew in Spark / large joins (Goldman)](#medium-4--data-skew-in-spark--large-joins-goldman)
- [HARD (2)](#hard-2)
   - [Hard 1 ‚Äî Design a pipeline for 100M events/day (Goldman)](#hard-1--design-a-pipeline-for-100m-eventsday-goldman)
   - [Hard 2 ‚Äî Reprocess only failed records (Goldman)](#hard-2--reprocess-only-failed-records-goldman)
- [Wrap / Next steps](#wrap--next-steps)


## Sources used for the questions / facts

* Glassdoor / candidate experience reports (Visa, Goldman Sachs) ‚Äî used as the source that these questions were asked. ([Glassdoor][1])
* Kafka delivery semantics & transactions (Confluent docs). ([Confluent Documentation][2])
* General Kafka ordering references (docs / blogs) and common Q&A. ([dattell.com][3])
* Spark troubleshooting / internals references (community guides & Q&A). ([InterviewBit][4])

---

# EASY (2)

### Easy 1 ‚Äî Spark inferSchema at scale (from Visa interview reports)

**Exact phrasing (reported):**

> ‚ÄúHow would you read **1 TB** file using Spark‚Äôs `inferSchema` option?‚Äù (reported from Visa interview logs). ([Glassdoor][1])

**What interviewer evaluates:**

* Practical Spark knowledge (schema inference pitfalls)
* Cost/IO tradeoffs and safe engineering practices for production jobs
* Ability to explain an operational approach (memory, parallelism, sampling)

**Answer approach (structure):**

1. One-line risk: `inferSchema` causes driver memory/OOM if used naively on huge files.
2. Option list: sampling, explicit schema, external schema registry, schema-on-read pipeline.
3. Show commands & implementation steps for the recommended approach.
4. Monitoring/validation & fallback.

**Model answer (say this on the call / whiteboard):**

1. **Short summary / risk:** `spark.read.option("inferSchema", "true").csv(path)` is convenient but **unsafe at 1 TB** ‚Äî Spark will try to sample or read partitions to infer types, and depending on how you call it the driver can become a hotspot or you can mis-infer types (strings vs numbers) for skewed data. So I wouldn‚Äôt call inferSchema blindly in production at that size. (1‚Äì2 lines)

2. **Recommended approach (practical steps):**

   * **Preferred:** *Provide an explicit schema* (best practice). If you know field names/types, declare a `StructType` and pass it to `spark.read.schema(mySchema).csv(path)`. This avoids any schema inference cost and gives correctness.
   * **If you don‚Äôt know schema fully:** Do a *controlled sampling*:

     1. Read a **small set of files / partitions** in parallel: e.g., `spark.read.csv(path, header=True).limit(100000)` or better: sample N files by listing the input directory and reading a representative file per partition. Use distributed sampling (not collect to driver).
     2. Use a schema inference library (or Spark‚Äôs `inferSchema` on that sample) to produce a schema, then validate against additional samples (edge-case detection).
     3. Use the validated schema in production reads.
   * **Alternative:** Use an external schema registry or glue catalog (Glue / Hive metastore / Avro/Parquet schema embedded) ‚Äî ingest producers should emit Parquet/Avro with schema to avoid inference entirely.
   * **Implementation snippet (PySpark):**

     ```python
     # sample-based approach
     sample_df = (spark.read.option("header","true")
                         .csv(sample_paths)             # small subset
                         .limit(200000))
     inferred = sample_df.schema
     # validate with another sample, then:
     df = spark.read.schema(inferred).csv(all_paths)
     ```

3. **Validation & monitoring:** Add schema checks in pipeline (column presence, nullable constraints), alert on schema drift, and keep a separate job to re-sample periodically (daily) to detect evolving schemas.

4. **Why this is safe:** explicit schema avoids driver pressure and reduces shuffle/serialization surprises. For 1 TB, avoid inferSchema on the whole dataset.

**Key tradeoffs / follow-ups:**

* If the schema changes frequently, invest into a schema-discovery pipeline + producer contracts (Avro/Protobuf) rather than repeated inference.
* If producers can‚Äôt add schema, use a streaming sampling job that maintains a canonical evolving schema.

---

### Easy 2 ‚Äî Consistency between Kafka and downstream data lake (Goldman report)

**Exact phrasing (reported):**

> ‚ÄúHow would you ensure data consistency between Kafka and a downstream data lake?‚Äù (Goldman Sachs interview report). ([Jointaro][5])

**What interviewer evaluates:**

* Knowledge of producer/consumer delivery semantics and offset management
* Practical approaches: transactional outbox, CDC, idempotent writes, exactly-once vs at-least-once tradeoffs
* Ability to propose monitoring and reconciliation strategies

**Answer approach (structure):**

1. Clarify constraints (read/consume model, tolerance for duplicates, sink transactional capability).
2. Present 3 practical approaches (ranked): Outbox+CDC, Kafka Transactions, Idempotent consumer writes + offsets.
3. Give the recommended design with sequence and monitoring.

**Model answer (say this):**

1. **Clarify (one sentence):** Are we allowed to change producers? Does the sink (data lake) support transactional writes? Are duplicates acceptable? ‚Äî assuming duplicates are not acceptable and sink is a typical object store (S3) or data lake table (Parquet/Iceberg), here are practical options.

2. **Options (summary):**

   * **Transactional outbox + CDC (recommended for relational sources):** write application state + an outbox row in the same DB transaction; Debezium (or similar) reads the outbox via CDC and writes to Kafka; downstream consumers read Kafka and write to the data lake idempotently. This ensures DB is truth; CDC guarantees every state change is emitted once. (This avoids a distributed 2PC).
   * **Kafka Transactions (if everything is Kafka-native):** if the data lake write can be expressed as Kafka writes (or if downstream uses Kafka Streams), Kafka producer transactions can atomically write messages and commit consumer offsets as part of a transaction. This delivers exactly-once among Kafka topics but doesn‚Äôt solve writing to non-Kafka sinks. Confluent docs show this pattern. ([Confluent Documentation][2])
   * **Idempotent consumer writes + offset management:** Consumer writes records to sink using idempotency keys (dedupe table or upsert keys), then commits offsets manually after durable write. This is simpler but requires dedupe state and retention management.

3. **Recommended design (outbox + CDC flow):**

   * App writes business record + outbox row in single DB transaction.
   * CDC (Debezium) streams committed outbox rows to Kafka.
   * Consumer reads Kafka ‚Üí writes to data lake (use atomic write patterns like writing to staging path + rename / Iceberg transactional writes).
   * Mark outbox rows as processed or rely on CDC offsets and idempotent Kafka writes to avoid duplicates.
   * Monitor CDC lag, Kafka consumer lag, file creation rates, and end-to-end latency.

4. **Monitoring & reconciliation:** Have a periodic reconciliation job that compares source (DB) counts/keys vs sink, alert on drift, and provide a replay path from Kafka/DB WAL.

**Key tradeoffs / follow-ups:**

* Outbox+CDC adds complexity but gives a strong correctness model for relational systems.
* Kafka transactions are great when the whole pipeline is Kafka-centric but don‚Äôt magically give atomicity for external systems like S3 without additional tooling.

*(Reference: the question appears in candidate reports for Goldman; Kafka transactions docs for EOS reference).)* ([Jointaro][5])

---

# MEDIUM (4)

### Medium 1 ‚Äî Spark OOM troubleshooting (Goldman)

**Exact phrasing (reported):**

> ‚ÄúYour Spark job is running out of memory. How do you diagnose and fix it?‚Äù (Goldman Sachs interview report). ([Glassdoor][6])

**What interviewer evaluates:**

* Practical debugging process (logs, metrics)
* Knowledge of Spark memory model (executor memory vs driver, storage vs execution memory, off-heap)
* Specific levers: serialization, partitioning, broadcast join, caching strategy, shuffle tuning

**Answer approach (structure):**

1. Clarify whether driver/executor OOM and workload characteristics.
2. Diagnostic checklist (metrics & logs to inspect).
3. Concrete fixes prioritized (data/algorithm changes first, then config).
4. Example commands/configs.

**Model answer (say this):**

1. **Clarify:** First I‚Äôd ask ‚Äî is it executor OOM or driver OOM? Is this batch or streaming? Is the job failing immediately on start or mid-stage? (one sentence ‚Äî shows disciplined approach)

2. **Diagnostics checklist (what to inspect):**

   * **Driver / Executor logs** (GC logs, OOM stack traces).
   * **Spark UI:** stage/task timelines, task memory spill, shuffle read/write sizes, storage memory usage, executor memory distribution.
   * **YARN / Kubernetes node metrics:** container memory limits, host OOMs, disk pressure.
   * **Data skew / partition sizes:** list input partition sizes (S3 file sizes, partitioning statistics).

3. **Fixes (ordered):**

   * **Algorithmic / data fixes (preferred):**

     * Reduce memory per task: increase parallelism (`spark.default.parallelism` or `repartition`) so partitions are smaller.
     * Avoid wide shuffles if possible: use broadcast join when one side is small (`broadcast(smallDF)`), or pre-aggregate on map side.
     * Handle skew: salting for hot keys or use `mapPartitions` to implement progressive aggregation.
   * **Serialization & memory config:**

     * Use Kryo serializer (`spark.serializer=org.apache.spark.serializer.KryoSerializer`) and register classes.
     * Tune executor memory and fraction: `spark.executor.memory`, `spark.memory.fraction`, `spark.memory.storageFraction`.
   * **Storage & spill tuning:**

     * Increase `spark.sql.shuffle.partitions` thoughtfully; enable spilling and tune `spark.shuffle.file.buffer`, compression for shuffle.
     * If using caching, ensure you use `persist(StorageLevel.DISK_ONLY)` for large intermediate data.
   * **Speculative exec & straggler mitigation:**

     * If a few slow tasks cause retries and memory blowups, consider `spark.speculation=true` and tune thresholds ‚Äî but speculative exec duplicates work, so use after root-cause analysis.
   * **Operational:** increase container memory limits or upgrade node types only after data/algorithm fixes.

4. **Concrete example (config snippets):**

   ```properties
   spark.serializer=org.apache.spark.serializer.KryoSerializer
   spark.sql.shuffle.partitions=2000   # increase if stage has too few partitions
   spark.executor.memory=8g
   spark.memory.fraction=0.6
   spark.memory.storageFraction=0.3
   ```

5. **Verification:** re-run on a smaller sample, monitor task memory / GC metrics (from Spark UI & Ganglia/Prometheus), and iterate.

**Key tradeoffs / follow-ups:**

* Increasing partitions reduces per-task memory but increases task scheduling overhead.
* Speculative exec can mask but not fix skew. Be ready to explain how you measure GC vs task CPU vs network IO.

(Authoritative Spark troubleshooting guides discuss these levers.) ([InterviewBit][4])

---

### Medium 2 ‚Äî Kafka consumer lag reduction without losing events (Goldman)

**Exact phrasing (reported):**

> ‚ÄúYou notice high consumer lag in a Kafka topic. How do you reduce lag without losing events?‚Äù (Goldman Sachs interview report). ([Glassdoor][6])

**What interviewer evaluates:**

* Understanding of Kafka consumer groups, partitioning, throughput limits
* Practical techniques: increase parallelism, tune batch size, investigate broker / network bottlenecks, consumer side processing optimizations, backpressure design

**Answer approach (structure):**

1. Confirm whether lag is producer or consumer bottleneck.
2. Quick wins (scale consumers, increase partition count).
3. Consumer tuning and safe patterns (idempotency, backpressure, throttling).
4. Longer-term fixes (data model, partitioning).

**Model answer (say this):**

1. **Confirm source:** First check whether lag is because producers flood topic (very high ingress), or consumers are slow (slow processing, IO, or GC), or brokers are overloaded. Check `kafka-consumer-groups.sh --describe` and broker metrics (CPU, network, I/O).

2. **Quick fixes (safe, minimal risk):**

   * **Scale consumers:** Increase consumer instances in the consumer group if partitions ‚â• consumers. If partitions < consumers, increase partition count (safe for many topics; rebalancing will happen). More partitions = more parallelism.
   * **Add more compute to consumers:** scale up the consumer pods/VMs (more CPU/memory) to process messages faster.
   * **Increase fetch sizes & batch processing:** tune `fetch.max.bytes`, `max.partition.fetch.bytes`, and process in batches (bulk DB writes) to amortize overhead.
   * **Use async IO / batching** in consumer processing pipeline to reduce per-message latency.

3. **Safe correctness measures (no event loss):**

   * **Disable auto-commit; do manual offset commits after durable processing** (commit only after the side-effect is safely persisted). This avoids data loss during catches.
   * **Idempotent sinks:** ensure your sink operations are idempotent or use upserts to avoid duplicates when reprocessing.
   * **Backpressure:** if consumers can‚Äôt keep up, consider applying throttling at the producer side (if possible) or queueing where you can persist incoming messages temporarily (e.g., increase retention/replay window).

4. **Longer-term:**

   * **Revisit partitioning key:** ensure hot partitions not causing skew. For hot keys, implement sharding or dedicated partitions with more consumer resources.
   * **Broker tuning:** check replica health, ISR, disk throughput ‚Äî if broker is slow, add brokers, tune replication and `min.insync.replicas` as needed.

5. **Monitoring:** track consumer lag per partition, broker network IO, consumer GC/CPU metrics. Add alerts for sustained lag > threshold and for rebalances which cause additional lag.

**Key tradeoffs / follow-ups:**

* Increasing partitions helps parallelism but increases metadata load and potential imbalance; keep partitions manageable.
* Committing offsets early (auto-commit) reduces lag but risks data loss ‚Äî do not do this if correctness matters.

(Confluent docs and operator guides show these configs and patterns.) ([Confluent Documentation][2])

---

### Medium 3 ‚Äî Handling schema evolution for JSON in Kafka (reported)

**Exact phrasing (reported):**

> ‚ÄúHow would you handle schema evolution dynamically while processing JSON data from Kafka?‚Äù (candidate reports). ([Jointaro][5])

**What interviewer evaluates:**

* Knowledge of schema management patterns (Avro/Schema Registry, Protobuf), backward/forward compatibility, runtime compatibility strategies.
* Practical ingestion pipeline design and migration steps.

**Answer approach (structure):**

1. Explain problem: JSON is schema-less leading to runtime surprises.
2. Present options (schema registry + typed formats; validation + evolution policy).
3. Implementation + example migration plan.

**Model answer (say this):**

1. **Problem:** JSON gives flexibility but no enforced schema, making downstream consumers brittle when fields change. We need predictable evolution rules and a discovery/validation mechanism.

2. **Options (recommended order):**

   * **Switch producers to a typed format + schema registry (Avro/Protobuf) ‚Äî best:** enforce producers register schemas in Confluent Schema Registry. Use compatibility settings (BACKWARD/ FORWARD/ FULL) so schema changes are validated. Consumers deserialize using registry and handle optional fields safely.
   * **If producers can‚Äôt change immediately:** use a *schema discovery layer* upstream:

     * Build a lightweight validation service (schema inference + canonicalization) that consumes raw JSON and emits canonical Avro messages to Kafka or writes canonical rows to sink.
     * Maintain a mapping table of schema versions; consumers use versioned deserializers.
   * **JSON Schema + validation:** producers publish a `schemaVersion` header; consumers validate JSON against stored JSON Schema for that version; consumers must be resilient to missing/new fields.

3. **Implementation details:**

   * **Schema registry flow:** producer includes `schemaId` or register new schema on change; consumer deserializes via registry. For Avro with Confluent: `io.confluent.kafka.serializers.KafkaAvroSerializer`.
   * **Compatibility policy:** choose `BACKWARD` if consumers must tolerate new optional fields; `FULL` if stricter checks needed.
   * **Migration:** phase rollouts ‚Äî start by validating producer changes in staging, ensure all consumers handle optional fields, then flip `compatibility` or accept new fields.

4. **Runtime handling:** add schema validation and alerting (if unknown schema version appears). Add a ‚Äúschema evolution playbook‚Äù for producers (how to add fields, rename, deprecate).

**Key tradeoffs / follow-ups:**

* Avro + Schema Registry adds operational overhead but is standard for production pipelines.
* JSON + schema discovery is faster short-term but increases long-term technical debt.

(See Debezium/Confluent best practices for schema management.) ([Confluent Documentation][2])

---

### Medium 4 ‚Äî Data skew in Spark / large joins (Goldman)

**Exact phrasing (reported):**

> ‚ÄúHow do you handle data skew in Spark? What are the best techniques for large joins?‚Äù (Goldman Sachs candidate reports). ([Glassdoor][6])

**What interviewer evaluates:**

* Knowledge of join strategies (broadcast, shuffle join, sort-merge), skew detection and mitigation (salting, map-side aggregation, adaptive execution)
* Ability to propose concrete, low-risk fixes and measure effectiveness

**Answer approach (structure):**

1. Briefly explain why skew is harmful (single partition overload).
2. Detection methods.
3. Mitigation techniques (ordered by preferability).
4. Example: salting strategy with code sketch.

**Model answer (say this):**

1. **Why it matters:** Skew occurs when a small set of keys own a large fraction of data; in joins this makes some reduce tasks huge and long-running, creating job tails and wasted resources.

2. **Detect skew:** use Spark UI to inspect task sizes (bytes read, records), or programmatically compute per-key counts on sample to find heavy hitters.

3. **Mitigations (practical):**

   * **Broadcast join**: if one side is small (`< broadcast threshold`), broadcast it (`broadcast(smallDF)`) to avoid shuffle.
   * **Salting (sharding heavy keys):** for heavy keys, replicate/partition them across k salted subkeys. Technique:

     * Add salt column to both sides for heavy keys: on the large side add random salt (0..k-1) to key; on small side replicate the record k times with each salt; join on `(key, salt)`, then aggregate back by key.
     * Example (PySpark sketch):

       ```python
       # identify heavy_keys set
       k=16
       big = big_df.withColumn("salt", when(col("key").isin(heavy_keys), floor(rand()*k)).otherwise(lit(0)))
       small = small_df.withColumn("salt", explode(array([lit(i) for i in range(k)]))) \
                      .filter((col("key").isin(heavy_keys)) | (col("salt")==0))
       joined = big.join(small, on=["key","salt"])
       ```
   * **Map-side pre-aggregate:** reduce data before the shuffle (e.g., aggregate in map stage).
   * **Adaptive Query Execution (AQE):** enable AQE in Spark 3.x to automatically coalesce skewed partitions and handle dynamic partition sizing.
   * **Repartition with custom partitioner:** choose partitioner that spreads heavy keys (hash + randomization).
   * **Use specialized joins:** e.g., **range join** or **sort-merge** tuned with `spark.sql.autoBroadcastJoinThreshold` and shuffle partitions.

4. **Why salting works:** it spreads a heavy key‚Äôs workload across multiple reducers; drawback is more joins/replication and complexity in re-aggregation.

5. **Measure success:** after change, check per-task durations and spilled bytes; aim to reduce max task time and shuffle spill.

**Key tradeoffs / follow-ups:**

* Salting increases data volume and join cost; prefer broadcast or pre-aggregation where possible.
* AQE simplifies operations but requires Spark 3.x and careful testing.

(These techniques are standard Spark best practices.) ([InterviewBit][4])

---

# HARD (2)

### Hard 1 ‚Äî Design a pipeline for 100M events/day (Goldman)

**Exact phrasing (reported):**

> ‚ÄúDesign a data pipeline to ingest, process, and store **100 million events per day**. Ensure it can handle spikes during peak hours.‚Äù (Goldman Sachs report). ([Jointaro][5])

**What interviewer evaluates:**

* End-to-end system design (scalability, fault tolerance, SLOs)
* Concrete choices for ingestion (Kafka/managed streaming), processing (Spark/Flink), storage (data lake / OLAP), partitioning & retention, monitoring & cost tradeoffs

**Answer approach (structure):**

1. Clarify requirements & SLOs (latency, durability, retention, query patterns).
2. High-level architecture diagram & components.
3. Detailed ingestion/processing/storage design with sizing rationale.
4. Spike handling strategy, failure/recovery, monitoring, cost considerations.

**Model answer (say this ‚Äî draw a 3-tier diagram while speaking):**

1. **Clarify assumptions (state these):**

   * 100M events/day ‚âà 1,157 events/sec average; peaks could be 10√ó (12k/s). Each event size ~1‚Äì2 KB. Retention: raw data 90 days; processed daily aggregates for years. Queries: ad hoc analytics, daily batch aggregation, occasional near-real-time alerts. Exactly-once not strictly required for analytics, but at least-once with dedupe is desired.

2. **High-level architecture (components / flow):**

   ```
   Producers -> API Gateway / Ingest (Kinesis or Kafka) -> Stream Processing (Spark Structured Streaming or Flink)
               -> Raw event store (S3 / Data Lake) + Processed outputs (Iceberg / Delta) -> OLAP (Presto / Redshift) 
               -> Monitoring + Metadata (Glue/Hive metastore)
   ```

   * **Ingest:** Use **Kafka** (self-managed or MSK) as durable, scalable buffer. Partition topic by `customer_id % P` to distribute load. Ensure retention long enough for reprocessing.
   * **Processing:** Use **Spark Structured Streaming** (micro-batch) or **Flink** (true stream). For windowed metrics, do incremental aggregations and write outputs to Iceberg/Delta on S3. Checkpointing enabled.
   * **Storage:** Raw events ‚Üí compressed Parquet on S3 (partitioned by date and shard). Processed tables ‚Üí Iceberg/Delta for ACID and snapshot queries. Expose to analysts via Presto/Trino.
   * **Serving / Lookups:** For low-latency point lookups, maintain a feature store (e.g., DynamoDB) updated by streaming jobs.

3. **Sizing & partitioning:**

   * For Kafka: decide partitions = `expected parallel consumers` * safety factor; e.g., if peak 12k/sec and a consumer can handle 1k/sec, need ~12 partitions per topic for consumers; add replication factor 3.
   * For Spark: choose number of executors to match parallelism of partitions; use autoscaling to handle spike windows. Use `spark.sql.shuffle.partitions` aligned with Kafka partitions.

4. **Spike handling strategy:**

   * **Buffering & throttling:** Kafka buffers spikes. Producers write to ingress frontends; if upstream overload, store temporarily in S3 (backpressure fallback).
   * **Autoscaling:** stream processing cluster autoscaling (Kubernetes or EMR auto-scale) based on lag/CPU/throughput.
   * **Graceful degradation:** during huge floods, switch to lower processing fidelity (sampled metrics) while continuing to store raw events for offline reprocessing.

5. **Failure & replay:** Kafka retention + S3 raw store enables replay. Processing jobs use checkpoint offsets; if reprocessing required, restart consumer from earlier offset.

6. **Deduplication & correctness:** include event idempotency keys in event payload; streaming job maintains a dedupe store (e.g., compacted Kafka topic or RocksDB state store with TTL) to dedupe at ingest.

7. **Monitoring & SLOs:** track producer success rate, Kafka lag, consumer throughput, processing latency percentiles (p50/p95/p99), S3 write rates, job checkpoint age. Alerts on sustained lag or failed commits.

8. **Cost considerations:** use S3 + compute ephemeral clusters (spot instances where acceptable) to minimize storage + compute cost; compress and partition raw events to reduce storage/read cost.

**Why this works:** decoupled buffer (Kafka) + scalable stream processing + durable raw store gives resilience to spikes and makes replay and reprocessing practical.

**Key tradeoffs / follow-ups:**

* Choice between Spark and Flink depends on latency needs and stateful processing requirements. Flink gives lower end-to-end latency.
* Exactly-once semantics increase complexity (Kafka transactions or transactional sinks). For analytics, at-least-once + dedupe often suffices.

(Question appears in Goldman candidate reports; the design above uses standard industry patterns.) ([Jointaro][5])

---

### Hard 2 ‚Äî Reprocess only failed records (Goldman)

**Exact phrasing (reported):**

> ‚ÄúIf one stage of your ETL pipeline fails, how would you ensure no data loss and reprocess only the failed records?‚Äù (Goldman Sachs candidate reports). ([Jointaro][5])

**What interviewer evaluates:**

* Practical reliability patterns (checkpointing, offsets, idempotency, partitioned processing)
* Ability to design reprocessing strategies with bounded rework & data guarantees

**Answer approach (structure):**

1. Explain failure detection & invariant (what counts as successful).
2. Propose architectural patterns: idempotent processing, checkpointed offsets, dead-letter / failed records topic, outbox pattern.
3. Provide reprocessing recipe with steps & commands.

**Model answer (say this):**

1. **Invariant:** Define success = downstream committed durable write (e.g., file persisted & metadata committed, or DB transaction committed). We must guarantee that every input either is processed exactly once or deterministically deduped.

2. **Patterns to use concurrently (recommended):**

   * **Checkpointing + Offsets:** Use Kafka consumer group offsets committed only after successful downstream write (manual commit). For Spark Structured Streaming, this is handled by checkpointing. Checkpoints record progress so on restart job resumes where it left off.
   * **Idempotent sinks:** Make writes idempotent (upserts keyed by event id) so reprocessing same event is harmless. For databases use primary key or idempotency key; for files use deterministic partition & file names (staging + atomic rename).
   * **Dead-letter / failed records topic:** On per-record failure (e.g., schema error), write the record to a DLQ topic with error metadata. This keeps pipeline moving while isolating bad rows.
   * **Transactional outbox / write-through patterns:** Ensure source or intermediate stage logs processed offsets and outcomes (e.g., results written alongside offsets into a durable store) to allow precise replay.

3. **Reprocess only failed records ‚Äî recipe:**

   * **If stage crash / restart:** job resumes from last committed checkpoint/offset‚Äîrecords after that are reprocessed. Because sinks are idempotent, duplicates are safe.
   * **If some records failed earlier and were written to DLQ:** run a targeted reprocessor that reads the DLQ, fixes or transforms records, then republished to main topic or wrote directly to sink (ensuring idempotency).
   * **If failure happened after partial writes:** track per-input processing status (e.g., add `processed` flag in a transactional metadata table keyed by event id); reprocess only those with `processed = false`.

4. **Concrete Spark/Kafka example (commands):**

   * Spark Structured Streaming with Kafka source + checkpoint:

     ```python
     df = (spark.readStream.format("kafka")
           .option("subscribe","topic")
           .load())
     # process and write to sink atomically using foreachBatch:
     def write_batch(batch_df, batch_id):
         # use idempotent upserts into DB or write to staging file then rename
         batch_df.write.format("parquet")...  # example
     query = (df.writeStream.foreachBatch(write_batch)
              .option("checkpointLocation","s3://checkpoints/jobA")
              .start())
     ```
   * DLQ: in `foreachBatch`, catch per-record exceptions and write to a DLQ Kafka topic with `producer.send(dlqTopic, record)`.

5. **Reconciliation & auditing:** run daily reconciliation comparing source counts vs sink counts per watermark partition; alert and backfill using retained Kafka offsets or raw S3 store.

**Why this works:** combination of checkpointed offsets + idempotent sinks + DLQ allows minimal rework and precise reprocessing.

**Key tradeoffs / follow-ups:**

* Tracking per-record processed flags adds storage overhead (but gives precise reprocessing ability).
* Idempotency requires design discipline (ids, upserts) and retention of dedupe keys.

---




[1]: https://www.glassdoor.com/Interview/How-would-you-read-1-TB-file-using-spark-s-inferschema-option-QTN_8512682.htm?utm_source=chatgpt.com "How would you read 1 TB file using spark's inferschema ..."
[2]: https://docs.confluent.io/kafka/design/delivery-semantics.html?utm_source=chatgpt.com "Message Delivery Guarantees for Apache Kafka"
[3]: https://dattell.com/data-architecture-blog/does-kafka-guarantee-message-order/?utm_source=chatgpt.com "Does Kafka Guarantee Message Order?"
[4]: https://www.interviewbit.com/spark-interview-questions/?utm_source=chatgpt.com "Top Spark Interview Questions (2025)"
[5]: https://www.jointaro.com/interviews/companies/goldman-sachs/experiences/data-engineering-united-wv-april-1-2025-no-offer-positive-e804c466/?utm_source=chatgpt.com "Data Engineering Interview Experience - United, West Virginia"
[6]: https://www.glassdoor.com/Interview/Goldman-Sachs-Data-Engineering-Interview-Questions-EI_IE2800.0%2C13_KO14%2C30.htm?utm_source=chatgpt.com "Goldman Sachs Data Engineering Interview Questions"
